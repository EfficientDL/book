{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[EDL]-Speech_Distillation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A7PPjQO862u"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQBII8Jae2he"
      },
      "source": [
        "# Creating the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBRRuoFmh4bm",
        "outputId": "405410b6-6bbf-4b45-887b-3288119e7b6d"
      },
      "source": [
        "!pip install pydub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/sh: line 1: pip: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS9OUfC3CrAn"
      },
      "source": [
        "read_config = tfds.ReadConfig(try_autocache=False)\n",
        "data_ds = tfds.load(\n",
        "    name='speech_commands',\n",
        "    read_config=read_config\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w354Fco4M8I2"
      },
      "source": [
        "def get_processed_ds(ds, num_examples, max_len=16000, num_classes=12):\n",
        "  \"\"\"Get the processed dataset.\"\"\"\n",
        "  i = 0\n",
        "  x = np.zeros([num_examples, max_len], dtype=np.float32)\n",
        "  y = np.zeros([num_examples, 1], dtype=np.uint8)\n",
        "\n",
        "  for i, example in enumerate(tfds.as_numpy(ds)):\n",
        "    audio_clip = example['audio']\n",
        "    label = example['label']\n",
        "\n",
        "    # Trim and pad the audio clips to the maximum length.\n",
        "    audio_clip = audio_clip[:max_len]\n",
        "    audio_clip = np.pad(audio_clip, (0, max_len - audio_clip.shape[0]))\n",
        "    audio_clip = np.reshape(audio_clip, (max_len)).astype(np.float32)\n",
        "    \n",
        "    x[i] = audio_clip\n",
        "    y[i] = label\n",
        "    \n",
        "    # i = i + 1\n",
        "    if i + 1 >= num_examples:\n",
        "      break\n",
        "\n",
        "  # STFT to extract the fourier transform on sliding windows of the input.\n",
        "  # Apply STFT on the audio data, but keep only the magnitude.\n",
        "  x = tf.abs(tf.signal.stft(x, frame_length=256, frame_step=128))\n",
        "\n",
        "  # Convert the labels to a one-hot vector.\n",
        "  y = np_utils.to_categorical(y, num_classes)\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ck4R9goYykl"
      },
      "source": [
        "x_train, y_train = get_processed_ds(data_ds['train'], 16000)\n",
        "x_test, y_test = get_processed_ds(data_ds['test'], 4890)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jff9BxI7e9l8",
        "outputId": "a96a7be2-eeaa-4852-ecf8-0df2c8c410b9"
      },
      "source": [
        "print(x_train.shape, y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16000, 124, 129) (16000, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf8ZMrdcfB0H",
        "outputId": "471cfd64-0147-4d66-be2d-3ce532afbc03"
      },
      "source": [
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4890, 124, 129) (4890, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui_RKNiDcl31",
        "outputId": "24857325-b8ff-452e-9281-9cb8f405dcdc"
      },
      "source": [
        "input_shape = tuple(x_train.shape.as_list()[1:])\n",
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(124, 129)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq01SB85gM5G"
      },
      "source": [
        "# Setting up the model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC3bwcffayQf"
      },
      "source": [
        "import os\n",
        "\n",
        "# Now let us create a callback for saving the best checkpoint so far.\n",
        "# It tries to find the checkpoint with the maximum categorical accuracy.\n",
        "# We will provide this to the model.fit function.\n",
        "\n",
        "CHECKPOINTS_DIR='checkpoints'\n",
        "\n",
        "# Create a directory for the checkpoints.\n",
        "!mkdir -p $CHECKPOINTS_DIR\n",
        "\n",
        "def best_checkpoint_callback(model_name):\n",
        "  checkpoint_dir_path = os.path.join(CHECKPOINTS_DIR, model_name)\n",
        "  if not os.path.exists(checkpoint_dir_path):\n",
        "    os.mkdir(checkpoint_dir_path)\n",
        "  checkpoint_path = os.path.join(checkpoint_dir_path, model_name)\n",
        "  return tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_categorical_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "def load_best_checkpoint(model, model_name):\n",
        "  checkpoint_dir_path = os.path.join(CHECKPOINTS_DIR, model_name)\n",
        "  checkpoint_path = os.path.join(checkpoint_dir_path, model_name)\n",
        "  model.load_weights(checkpoint_path)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3oBiGoAbvFf"
      },
      "source": [
        "def base_model(\n",
        "    num_classes=12,        # Number of classes in the model. \n",
        "    width_multiplier=1.0,  # Factor for scaling the network size.\n",
        "    params={},             # Additional hyper-params.\n",
        "):\n",
        "  # Use the width_multiplier for scaling the network, the larger the value,\n",
        "  # the larger the network. Keep a bound on the width though.\n",
        "  w = min(max(width_multiplier, 0.05), 3.0)\n",
        "\n",
        "  # inputs = keras.Input(shape=(16000,1))\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  x = inputs\n",
        "\n",
        "  # Create a regularizer to be used.\n",
        "  reg = keras.regularizers.l2(params.get('l2_reg_weight', 2e-4))\n",
        "\n",
        "  # Find the dropout rate for helping with further regularization.\n",
        "  dropout_rate = params.get('dropout_rate', 1.0)\n",
        "\n",
        "  # We will keep the first 'block' of layers scale invariant.\n",
        "  x = layers.Conv1D(\n",
        "      32, (9), padding='same', activation='relu', kernel_regularizer=reg)(\n",
        "          x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(\n",
        "      32, (9), padding='same', activation='relu', kernel_regularizer=reg)(\n",
        "          x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D(pool_size=(4))(x)\n",
        "  x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "  # A short hand for the round method.\n",
        "  r = lambda v: round(v)\n",
        "\n",
        "  # We will start to scale layers from here on.\n",
        "  x = layers.Conv1D(\n",
        "      # Note that the number of filters grows / shrinks with `w`.\n",
        "      r(64 * w), \n",
        "      (9), \n",
        "      padding='same',\n",
        "      activation='relu',\n",
        "      kernel_regularizer=reg)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(\n",
        "      r(64 * w), \n",
        "      (9),\n",
        "      padding='same',\n",
        "      activation='relu',\n",
        "      kernel_regularizer=reg)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D(pool_size=(4))(x)\n",
        "  x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "  x = layers.Conv1D(\n",
        "      r(128 * w), \n",
        "      (9),\n",
        "      padding='same',\n",
        "      activation='relu',\n",
        "      kernel_regularizer=reg)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(\n",
        "      r(128 * w), \n",
        "      (9),\n",
        "      padding='same',\n",
        "      activation='relu',\n",
        "      kernel_regularizer=reg)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D(pool_size=(4))(x)\n",
        "  x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dropout(dropout_rate)(x)\n",
        "  x = layers.Dense(r(512 * w), activation='relu', kernel_regularizer=reg)(x)\n",
        "  x = layers.Dropout(dropout_rate)(x)\n",
        "  x = layers.Dense(num_classes, kernel_regularizer=reg)(x)\n",
        "  logits = x\n",
        "  probabilities = layers.Activation('softmax')(logits)\n",
        "  outputs = probabilities\n",
        "\n",
        "  return keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXU7QJxIcAve"
      },
      "source": [
        "def get_compiled_model(\n",
        "    width_multiplier,\n",
        "    num_classes=12,\n",
        "    params={}):\n",
        "  \"\"\"Helper method to create the compiled model.\"\"\"  \n",
        "  learning_rate = params.get('learning_rate', 1e-3)\n",
        "\n",
        "  model = base_model(\n",
        "      num_classes, width_multiplier=width_multiplier, params=params)\n",
        "\n",
        "  opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt, \n",
        "                metrics='categorical_accuracy')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgC-JqxTcFf2"
      },
      "source": [
        "def get_model_name(width_multiplier, distillation=False):\n",
        "  \"\"\"A helper method to create a unique identifier for a model.\"\"\"\n",
        "  return 'wm_{}{}'.format(\n",
        "      str(width_multiplier).replace('.', ''),\n",
        "      ('_dist' if distillation else '')\n",
        "  )\n",
        "\n",
        "def train_model(width_multiplier, params={}, batch_size=128, epochs=100):\n",
        "  \"\"\"Train a solo model.\"\"\"\n",
        "  model_name = get_model_name(width_multiplier, distillation=False)\n",
        "\n",
        "  model = get_compiled_model(\n",
        "      width_multiplier=width_multiplier,\n",
        "      params=params)\n",
        "  model.summary()\n",
        "\n",
        "  model_history = model.fit(\n",
        "      x_train, \n",
        "      y_train, \n",
        "      batch_size=batch_size, \n",
        "      epochs=epochs, \n",
        "      validation_data=(x_test, y_test),\n",
        "      callbacks=[best_checkpoint_callback(model_name)],\n",
        "      shuffle=True)\n",
        "\n",
        "  return model, model_history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjJ2rsUtgRqk"
      },
      "source": [
        "# Training models solo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8-O35EjgtgT"
      },
      "source": [
        "## Train a large model with `width_multiplier`=1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZzNFk1OcbHN",
        "outputId": "ce5d4fe3-47f1-43b4-93ff-9344258a460e"
      },
      "source": [
        "width_multiplier = 1.0\n",
        "params = {\n",
        "    'learning_rate': 1e-3,\n",
        "    'dropout_rate': 0.5,\n",
        "}\n",
        "model_wm_10, _ = train_model(\n",
        "    width_multiplier, params, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 124, 129)]        0         \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 124, 32)           37184     \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 124, 32)          128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 124, 32)           9248      \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 124, 32)          128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPooling  (None, 31, 32)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 31, 32)            0         \n",
            "                                                                 \n",
            " conv1d_20 (Conv1D)          (None, 31, 64)            18496     \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 31, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_21 (Conv1D)          (None, 31, 64)            36928     \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 31, 64)           256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPoolin  (None, 7, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 7, 64)             0         \n",
            "                                                                 \n",
            " conv1d_22 (Conv1D)          (None, 7, 128)            73856     \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 7, 128)           512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_23 (Conv1D)          (None, 7, 128)            147584    \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 7, 128)           512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPoolin  (None, 1, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               66048     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 12)                6156      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 12)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 397,292\n",
            "Trainable params: 396,396\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "125/125 [==============================] - 3s 12ms/step - loss: 1.9565 - categorical_accuracy: 0.5967 - val_loss: 2.7905 - val_categorical_accuracy: 0.0838\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.5658 - categorical_accuracy: 0.6339 - val_loss: 2.5553 - val_categorical_accuracy: 0.0998\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.4094 - categorical_accuracy: 0.6350 - val_loss: 2.1908 - val_categorical_accuracy: 0.1951\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.2750 - categorical_accuracy: 0.6519 - val_loss: 1.8385 - val_categorical_accuracy: 0.3329\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.1528 - categorical_accuracy: 0.6779 - val_loss: 1.6321 - val_categorical_accuracy: 0.4599\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.0368 - categorical_accuracy: 0.7000 - val_loss: 1.3625 - val_categorical_accuracy: 0.5503\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.9477 - categorical_accuracy: 0.7286 - val_loss: 1.2384 - val_categorical_accuracy: 0.6145\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.8683 - categorical_accuracy: 0.7548 - val_loss: 1.0846 - val_categorical_accuracy: 0.6824\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.8064 - categorical_accuracy: 0.7731 - val_loss: 0.8913 - val_categorical_accuracy: 0.7499\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.7643 - categorical_accuracy: 0.7931 - val_loss: 0.8964 - val_categorical_accuracy: 0.7622\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.7280 - categorical_accuracy: 0.8056 - val_loss: 0.8659 - val_categorical_accuracy: 0.7524\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.6817 - categorical_accuracy: 0.8249 - val_loss: 0.7990 - val_categorical_accuracy: 0.7767\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.6575 - categorical_accuracy: 0.8353 - val_loss: 0.7665 - val_categorical_accuracy: 0.7902\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.6250 - categorical_accuracy: 0.8426 - val_loss: 0.7295 - val_categorical_accuracy: 0.8043\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.6065 - categorical_accuracy: 0.8509 - val_loss: 0.7904 - val_categorical_accuracy: 0.7935\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.5813 - categorical_accuracy: 0.8604 - val_loss: 0.6524 - val_categorical_accuracy: 0.8262\n",
            "Epoch 17/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.5668 - categorical_accuracy: 0.8644 - val_loss: 0.6011 - val_categorical_accuracy: 0.8481\n",
            "Epoch 18/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.5464 - categorical_accuracy: 0.8752 - val_loss: 0.7680 - val_categorical_accuracy: 0.7953\n",
            "Epoch 19/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.5329 - categorical_accuracy: 0.8779 - val_loss: 0.6724 - val_categorical_accuracy: 0.8346\n",
            "Epoch 20/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.5131 - categorical_accuracy: 0.8821 - val_loss: 0.6643 - val_categorical_accuracy: 0.8325\n",
            "Epoch 21/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.5167 - categorical_accuracy: 0.8827 - val_loss: 0.5916 - val_categorical_accuracy: 0.8540\n",
            "Epoch 22/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.5110 - categorical_accuracy: 0.8842 - val_loss: 0.6172 - val_categorical_accuracy: 0.8466\n",
            "Epoch 23/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4995 - categorical_accuracy: 0.8905 - val_loss: 0.6618 - val_categorical_accuracy: 0.8481\n",
            "Epoch 24/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4785 - categorical_accuracy: 0.8956 - val_loss: 0.5782 - val_categorical_accuracy: 0.8646\n",
            "Epoch 25/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4677 - categorical_accuracy: 0.9022 - val_loss: 0.6428 - val_categorical_accuracy: 0.8481\n",
            "Epoch 26/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4692 - categorical_accuracy: 0.8978 - val_loss: 0.6759 - val_categorical_accuracy: 0.8374\n",
            "Epoch 27/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4534 - categorical_accuracy: 0.9031 - val_loss: 0.5702 - val_categorical_accuracy: 0.8661\n",
            "Epoch 28/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4543 - categorical_accuracy: 0.9027 - val_loss: 0.5989 - val_categorical_accuracy: 0.8626\n",
            "Epoch 29/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4548 - categorical_accuracy: 0.9022 - val_loss: 0.5737 - val_categorical_accuracy: 0.8605\n",
            "Epoch 30/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4429 - categorical_accuracy: 0.9069 - val_loss: 0.5926 - val_categorical_accuracy: 0.8609\n",
            "Epoch 31/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4459 - categorical_accuracy: 0.9067 - val_loss: 0.5770 - val_categorical_accuracy: 0.8648\n",
            "Epoch 32/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4372 - categorical_accuracy: 0.9097 - val_loss: 0.6060 - val_categorical_accuracy: 0.8658\n",
            "Epoch 33/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4312 - categorical_accuracy: 0.9128 - val_loss: 0.6030 - val_categorical_accuracy: 0.8613\n",
            "Epoch 34/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4393 - categorical_accuracy: 0.9093 - val_loss: 0.6143 - val_categorical_accuracy: 0.8601\n",
            "Epoch 35/50\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.4241 - categorical_accuracy: 0.9159 - val_loss: 0.5186 - val_categorical_accuracy: 0.8881\n",
            "Epoch 36/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4307 - categorical_accuracy: 0.9094 - val_loss: 0.5635 - val_categorical_accuracy: 0.8771\n",
            "Epoch 37/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4246 - categorical_accuracy: 0.9151 - val_loss: 0.5752 - val_categorical_accuracy: 0.8708\n",
            "Epoch 38/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4138 - categorical_accuracy: 0.9180 - val_loss: 0.5374 - val_categorical_accuracy: 0.8773\n",
            "Epoch 39/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4079 - categorical_accuracy: 0.9171 - val_loss: 0.6498 - val_categorical_accuracy: 0.8542\n",
            "Epoch 40/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4015 - categorical_accuracy: 0.9219 - val_loss: 0.5536 - val_categorical_accuracy: 0.8806\n",
            "Epoch 41/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4038 - categorical_accuracy: 0.9216 - val_loss: 0.6187 - val_categorical_accuracy: 0.8618\n",
            "Epoch 42/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.3999 - categorical_accuracy: 0.9209 - val_loss: 0.5594 - val_categorical_accuracy: 0.8773\n",
            "Epoch 43/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.3932 - categorical_accuracy: 0.9227 - val_loss: 0.6384 - val_categorical_accuracy: 0.8714\n",
            "Epoch 44/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4062 - categorical_accuracy: 0.9216 - val_loss: 0.6030 - val_categorical_accuracy: 0.8699\n",
            "Epoch 45/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4012 - categorical_accuracy: 0.9215 - val_loss: 0.5761 - val_categorical_accuracy: 0.8787\n",
            "Epoch 46/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4004 - categorical_accuracy: 0.9229 - val_loss: 0.6406 - val_categorical_accuracy: 0.8626\n",
            "Epoch 47/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.3982 - categorical_accuracy: 0.9249 - val_loss: 0.5805 - val_categorical_accuracy: 0.8753\n",
            "Epoch 48/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4057 - categorical_accuracy: 0.9227 - val_loss: 0.6020 - val_categorical_accuracy: 0.8751\n",
            "Epoch 49/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.3874 - categorical_accuracy: 0.9286 - val_loss: 0.5327 - val_categorical_accuracy: 0.8861\n",
            "Epoch 50/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.3914 - categorical_accuracy: 0.9258 - val_loss: 0.6180 - val_categorical_accuracy: 0.8579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKfMtOyUrVGo"
      },
      "source": [
        "# Create a new Model object from the previously trained model with just the \n",
        "# logits layer. We will copy it's weights from the best checkpoint so far.\n",
        "# This model will be used as the teacher, to extract the logits.\n",
        "model_wm_10_without_softmax = tf.keras.Model(\n",
        "    inputs=model_wm_10.inputs,\n",
        "    outputs=model_wm_10.layers[-2].output)\n",
        "model_wm_10_without_softmax.compile(metrics='categorical_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shlhSuP7HHrR",
        "outputId": "2337fda2-c5ce-4db1-96c2-f5c307d1625d"
      },
      "source": [
        "# Load the weights and evaluate the model.\n",
        "# Even though the previous Model object's output was probabilities, and this new\n",
        "# model's output are the logits, it is essentially the same because the logits\n",
        "# will retain the same order as the probabilities. Thus, for the accuracy metric\n",
        "# they are identical.\n",
        "model_wm_10_without_softmax = load_best_checkpoint(\n",
        "    model_wm_10_without_softmax, \n",
        "    get_model_name(1.0))\n",
        "model_wm_10_without_softmax.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153/153 [==============================] - 1s 2ms/step - loss: 0.1395 - categorical_accuracy: 0.8881\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13952623307704926, 0.8881390690803528]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjReW1TMgX07"
      },
      "source": [
        "## Train a smaller model with `width_multiplier`=0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e3KgiwJud7T",
        "outputId": "2f52daf8-3a75-44d6-d084-adafd2381353"
      },
      "source": [
        "# Now let's train a solo model, with width_multiplier=0.1.\n",
        "width_multiplier = 0.1\n",
        "params = {\n",
        "    'learning_rate': 1e-3,\n",
        "    'dropout_rate': 0.2,\n",
        "}\n",
        "model_wm_01, model_wm_01_history = train_model(width_multiplier, params, \n",
        "                                               epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 124, 129)]        0         \n",
            "                                                                 \n",
            " conv1d_24 (Conv1D)          (None, 124, 32)           37184     \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 124, 32)          128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_25 (Conv1D)          (None, 124, 32)           9248      \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 124, 32)          128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_12 (MaxPoolin  (None, 31, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 31, 32)            0         \n",
            "                                                                 \n",
            " conv1d_26 (Conv1D)          (None, 31, 6)             1734      \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 31, 6)            24        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_27 (Conv1D)          (None, 31, 6)             330       \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 31, 6)            24        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_13 (MaxPoolin  (None, 7, 6)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 7, 6)              0         \n",
            "                                                                 \n",
            " conv1d_28 (Conv1D)          (None, 7, 13)             715       \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 7, 13)            52        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_29 (Conv1D)          (None, 7, 13)             1534      \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 7, 13)            52        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_14 (MaxPoolin  (None, 1, 13)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 1, 13)             0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 13)                0         \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 13)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 51)                714       \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 51)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                624       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 12)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,491\n",
            "Trainable params: 52,287\n",
            "Non-trainable params: 204\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "125/125 [==============================] - 2s 10ms/step - loss: 1.9992 - categorical_accuracy: 0.5021 - val_loss: 2.5884 - val_categorical_accuracy: 0.0834\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.5421 - categorical_accuracy: 0.6296 - val_loss: 2.7895 - val_categorical_accuracy: 0.0865\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.4237 - categorical_accuracy: 0.6322 - val_loss: 2.7617 - val_categorical_accuracy: 0.1061\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.3274 - categorical_accuracy: 0.6383 - val_loss: 3.0722 - val_categorical_accuracy: 0.0984\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.2544 - categorical_accuracy: 0.6440 - val_loss: 3.1157 - val_categorical_accuracy: 0.1233\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.1779 - categorical_accuracy: 0.6522 - val_loss: 2.4520 - val_categorical_accuracy: 0.1746\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0998 - categorical_accuracy: 0.6631 - val_loss: 2.0909 - val_categorical_accuracy: 0.2552\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0329 - categorical_accuracy: 0.6724 - val_loss: 2.0465 - val_categorical_accuracy: 0.2564\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.9785 - categorical_accuracy: 0.6861 - val_loss: 1.9598 - val_categorical_accuracy: 0.3076\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.9323 - categorical_accuracy: 0.6963 - val_loss: 1.8040 - val_categorical_accuracy: 0.3800\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.8886 - categorical_accuracy: 0.7136 - val_loss: 1.7418 - val_categorical_accuracy: 0.4025\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.8571 - categorical_accuracy: 0.7212 - val_loss: 1.6215 - val_categorical_accuracy: 0.4425\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.8285 - categorical_accuracy: 0.7262 - val_loss: 1.4776 - val_categorical_accuracy: 0.5188\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.8020 - categorical_accuracy: 0.7411 - val_loss: 1.4049 - val_categorical_accuracy: 0.5472\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.7672 - categorical_accuracy: 0.7471 - val_loss: 1.3763 - val_categorical_accuracy: 0.5550\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.7529 - categorical_accuracy: 0.7547 - val_loss: 1.3992 - val_categorical_accuracy: 0.5515\n",
            "Epoch 17/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.7223 - categorical_accuracy: 0.7625 - val_loss: 1.3270 - val_categorical_accuracy: 0.5804\n",
            "Epoch 18/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.7033 - categorical_accuracy: 0.7707 - val_loss: 1.2685 - val_categorical_accuracy: 0.6084\n",
            "Epoch 19/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6963 - categorical_accuracy: 0.7754 - val_loss: 1.3763 - val_categorical_accuracy: 0.5926\n",
            "Epoch 20/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6637 - categorical_accuracy: 0.7850 - val_loss: 1.3011 - val_categorical_accuracy: 0.6000\n",
            "Epoch 21/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6447 - categorical_accuracy: 0.7923 - val_loss: 1.2566 - val_categorical_accuracy: 0.5953\n",
            "Epoch 22/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6435 - categorical_accuracy: 0.7928 - val_loss: 1.2549 - val_categorical_accuracy: 0.6196\n",
            "Epoch 23/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6303 - categorical_accuracy: 0.7984 - val_loss: 1.2560 - val_categorical_accuracy: 0.6264\n",
            "Epoch 24/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6238 - categorical_accuracy: 0.8019 - val_loss: 1.3206 - val_categorical_accuracy: 0.5861\n",
            "Epoch 25/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.6105 - categorical_accuracy: 0.8062 - val_loss: 1.2989 - val_categorical_accuracy: 0.6115\n",
            "Epoch 26/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5977 - categorical_accuracy: 0.8050 - val_loss: 1.1862 - val_categorical_accuracy: 0.6286\n",
            "Epoch 27/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5932 - categorical_accuracy: 0.8084 - val_loss: 1.1304 - val_categorical_accuracy: 0.6571\n",
            "Epoch 28/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5773 - categorical_accuracy: 0.8163 - val_loss: 1.0618 - val_categorical_accuracy: 0.6661\n",
            "Epoch 29/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5726 - categorical_accuracy: 0.8197 - val_loss: 0.9917 - val_categorical_accuracy: 0.6894\n",
            "Epoch 30/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5688 - categorical_accuracy: 0.8188 - val_loss: 1.0511 - val_categorical_accuracy: 0.6785\n",
            "Epoch 31/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5530 - categorical_accuracy: 0.8229 - val_loss: 1.1878 - val_categorical_accuracy: 0.6515\n",
            "Epoch 32/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5483 - categorical_accuracy: 0.8256 - val_loss: 1.1243 - val_categorical_accuracy: 0.6785\n",
            "Epoch 33/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5455 - categorical_accuracy: 0.8273 - val_loss: 1.0307 - val_categorical_accuracy: 0.6816\n",
            "Epoch 34/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5389 - categorical_accuracy: 0.8303 - val_loss: 1.0761 - val_categorical_accuracy: 0.6875\n",
            "Epoch 35/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5336 - categorical_accuracy: 0.8313 - val_loss: 1.0475 - val_categorical_accuracy: 0.6888\n",
            "Epoch 36/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5457 - categorical_accuracy: 0.8274 - val_loss: 0.9492 - val_categorical_accuracy: 0.7065\n",
            "Epoch 37/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5325 - categorical_accuracy: 0.8294 - val_loss: 1.2661 - val_categorical_accuracy: 0.6509\n",
            "Epoch 38/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5235 - categorical_accuracy: 0.8335 - val_loss: 0.9677 - val_categorical_accuracy: 0.7094\n",
            "Epoch 39/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5306 - categorical_accuracy: 0.8328 - val_loss: 1.1793 - val_categorical_accuracy: 0.6628\n",
            "Epoch 40/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5155 - categorical_accuracy: 0.8414 - val_loss: 1.0805 - val_categorical_accuracy: 0.6759\n",
            "Epoch 41/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5187 - categorical_accuracy: 0.8343 - val_loss: 1.0296 - val_categorical_accuracy: 0.7039\n",
            "Epoch 42/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5092 - categorical_accuracy: 0.8404 - val_loss: 1.1041 - val_categorical_accuracy: 0.7016\n",
            "Epoch 43/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5039 - categorical_accuracy: 0.8436 - val_loss: 1.1543 - val_categorical_accuracy: 0.6945\n",
            "Epoch 44/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5085 - categorical_accuracy: 0.8420 - val_loss: 1.0776 - val_categorical_accuracy: 0.6834\n",
            "Epoch 45/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.5044 - categorical_accuracy: 0.8441 - val_loss: 1.0748 - val_categorical_accuracy: 0.6787\n",
            "Epoch 46/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.4959 - categorical_accuracy: 0.8461 - val_loss: 0.9994 - val_categorical_accuracy: 0.7072\n",
            "Epoch 47/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.4953 - categorical_accuracy: 0.8502 - val_loss: 0.9697 - val_categorical_accuracy: 0.7143\n",
            "Epoch 48/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.4963 - categorical_accuracy: 0.8506 - val_loss: 1.0097 - val_categorical_accuracy: 0.7166\n",
            "Epoch 49/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.4738 - categorical_accuracy: 0.8536 - val_loss: 1.0773 - val_categorical_accuracy: 0.6845\n",
            "Epoch 50/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.4904 - categorical_accuracy: 0.8500 - val_loss: 1.0990 - val_categorical_accuracy: 0.6908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ovPy7Uzvt29",
        "outputId": "c13b2e36-ee1b-4e5f-f2c6-7d937954505d"
      },
      "source": [
        "# Let us find the best checkpoint and compute its accuracy.\n",
        "model_wm_01 = load_best_checkpoint(model_wm_01, get_model_name(0.1))\n",
        "model_wm_01.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153/153 [==============================] - 0s 2ms/step - loss: 1.0097 - categorical_accuracy: 0.7166\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0096887350082397, 0.716564416885376]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz4JmoZAoElv"
      },
      "source": [
        "# Setting up distillation training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CchS13Xrgz-j"
      },
      "source": [
        "### Establishing our teacher model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS9MHLdtcSe0"
      },
      "source": [
        "teacher_model = model_wm_10_without_softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "450Mb9SOxq4W"
      },
      "source": [
        "# Create the distillation labels by stacking the teacher model's outputs \n",
        "# and the ground-truth one-hot labels together.\n",
        "def get_distillation_labels(teacher_model_instance, x_ds, y_ds):\n",
        "  \"\"\"Add the teacher logits to the label tensor.\"\"\"\n",
        "  # Generate the teacher model's logits.\n",
        "  teacher_logits = teacher_model_instance.predict(x_ds)\n",
        "  # Create the stacked tensor.\n",
        "  y_dist_ds = tf.stack([y_ds, teacher_logits], axis=1)\n",
        "  return y_dist_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KaWdUlx8GP4"
      },
      "source": [
        "y_dist_train = get_distillation_labels(teacher_model, x_train, y_train)\n",
        "y_dist_test = get_distillation_labels(teacher_model, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kQMnHQz-pCz"
      },
      "source": [
        "def get_compiled_distillation_model(\n",
        "    width_multiplier=1.0,\n",
        "    num_classes=12,\n",
        "    params={}):\n",
        "  learning_rate = params.get('learning_rate', 1e-3)\n",
        "  temperature = params.get('temperature', 1.0)\n",
        "  distillation_weight = params.get('distillation_weight', 1.0)\n",
        "\n",
        "  model = base_model(\n",
        "      num_classes, width_multiplier=width_multiplier, params=params)\n",
        "\n",
        "  def distillation_loss_fn(y_true_combined, y_pred):\n",
        "    \"\"\"Custom distillation loss function.\"\"\"\n",
        "    # We will split the y tensor to extract the ground-truth and the teacher's\n",
        "    # soft-labels.\n",
        "    y_true_split = tf.split(y_true_combined, 2, axis=1)\n",
        "    \n",
        "    # Create the ground-truth loss function as usual.\n",
        "    ground_truth = tf.squeeze(y_true_split[0], axis=1)\n",
        "    gt_loss = tf.keras.losses.CategoricalCrossentropy()(\n",
        "      ground_truth, y_pred)\n",
        "    \n",
        "    # Create the distillation loss using the soft-labels.\n",
        "    logits = tf.squeeze(y_true_split[1], axis=1)\n",
        "    soft_labels = tf.nn.softmax(logits / temperature)\n",
        "    dist_loss = tf.keras.losses.CategoricalCrossentropy()(\n",
        "        soft_labels, y_pred)\n",
        "\n",
        "    # Combine the two into a single loss function.\n",
        "    loss = gt_loss + distillation_weight * dist_loss * (temperature ** 2.0)\n",
        "    return loss\n",
        "  \n",
        "  def categorical_accuracy(labels, model_pred):\n",
        "    # Extract the ground-truth labels.\n",
        "    ground_truth = tf.split(labels, 2, axis=1)[0]\n",
        "    ground_truth = tf.squeeze(ground_truth, axis=1)\n",
        "\n",
        "    # Compute the accuracy as usual.\n",
        "    return tf.keras.metrics.categorical_accuracy(ground_truth, model_pred)\n",
        "\n",
        "  opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "  # Compile the model with the custom loss function and metric.\n",
        "  model.compile(\n",
        "      loss=distillation_loss_fn,\n",
        "      metrics=[categorical_accuracy],\n",
        "      optimizer=opt)\n",
        "  return model\n",
        "\n",
        "def train_dist_model(width_multiplier, params={}, batch_size=128, epochs=100):\n",
        "  model_name = get_model_name(width_multiplier, distillation=True)\n",
        "\n",
        "  model = get_compiled_distillation_model(\n",
        "      width_multiplier=width_multiplier,\n",
        "      params=params)\n",
        "  model.summary()\n",
        "\n",
        "  # The change here would be that we are using the distillation labels.\n",
        "  model_history = model.fit(\n",
        "      x_train, \n",
        "      y_dist_train, \n",
        "      batch_size=batch_size, \n",
        "      epochs=epochs, \n",
        "      validation_data=(x_test, y_dist_test),\n",
        "      callbacks=[best_checkpoint_callback(model_name)],\n",
        "      shuffle=True)\n",
        "\n",
        "  return model, model_history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imIj0ej7gisi"
      },
      "source": [
        "# Train the `width_multiplier`=0.1 model with distillation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZo6PPhSFeMi",
        "outputId": "ea8536d1-0487-4e0c-f93e-770acd3d336a"
      },
      "source": [
        "# Train a model with width_multiplier=0.1 in a distillation setting.\n",
        "width_multiplier = 0.1\n",
        "params = {\n",
        "    'learning_rate': 1e-3,\n",
        "    'dropout_rate': 0.2,\n",
        "    'distillation_weight': 1.0,\n",
        "}\n",
        "model_wm_01_dist, model_wm_01_dist_history = train_dist_model(\n",
        "    width_multiplier, params, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 124, 129)]        0         \n",
            "                                                                 \n",
            " conv1d_30 (Conv1D)          (None, 124, 32)           37184     \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 124, 32)          128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_31 (Conv1D)          (None, 124, 32)           9248      \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 124, 32)          128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_15 (MaxPoolin  (None, 31, 32)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 31, 32)            0         \n",
            "                                                                 \n",
            " conv1d_32 (Conv1D)          (None, 31, 6)             1734      \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 31, 6)            24        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_33 (Conv1D)          (None, 31, 6)             330       \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 31, 6)            24        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_16 (MaxPoolin  (None, 7, 6)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 7, 6)              0         \n",
            "                                                                 \n",
            " conv1d_34 (Conv1D)          (None, 7, 13)             715       \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 7, 13)            52        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1d_35 (Conv1D)          (None, 7, 13)             1534      \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 7, 13)            52        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_17 (MaxPoolin  (None, 1, 13)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 1, 13)             0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 13)                0         \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 13)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 51)                714       \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 51)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 12)                624       \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 12)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,491\n",
            "Trainable params: 52,287\n",
            "Non-trainable params: 204\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "125/125 [==============================] - 2s 10ms/step - loss: 3.8749 - categorical_accuracy: 0.5370 - val_loss: 5.0434 - val_categorical_accuracy: 0.0832\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 3.1005 - categorical_accuracy: 0.6312 - val_loss: 5.0656 - val_categorical_accuracy: 0.0853\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 2.8584 - categorical_accuracy: 0.6349 - val_loss: 5.2351 - val_categorical_accuracy: 0.1020\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 2.6707 - categorical_accuracy: 0.6421 - val_loss: 4.6577 - val_categorical_accuracy: 0.1618\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 2.4777 - categorical_accuracy: 0.6524 - val_loss: 4.4006 - val_categorical_accuracy: 0.1943\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 2.3327 - categorical_accuracy: 0.6609 - val_loss: 4.1080 - val_categorical_accuracy: 0.2194\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 2.1753 - categorical_accuracy: 0.6749 - val_loss: 3.6937 - val_categorical_accuracy: 0.2810\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 2.1005 - categorical_accuracy: 0.6787 - val_loss: 3.2336 - val_categorical_accuracy: 0.3624\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.9633 - categorical_accuracy: 0.6948 - val_loss: 3.0836 - val_categorical_accuracy: 0.4174\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.8736 - categorical_accuracy: 0.7053 - val_loss: 2.9292 - val_categorical_accuracy: 0.4528\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.7932 - categorical_accuracy: 0.7181 - val_loss: 2.5650 - val_categorical_accuracy: 0.5182\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.7184 - categorical_accuracy: 0.7310 - val_loss: 2.7071 - val_categorical_accuracy: 0.5123\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.6738 - categorical_accuracy: 0.7376 - val_loss: 2.2671 - val_categorical_accuracy: 0.5763\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.6306 - categorical_accuracy: 0.7438 - val_loss: 2.2270 - val_categorical_accuracy: 0.6006\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.5864 - categorical_accuracy: 0.7482 - val_loss: 2.2483 - val_categorical_accuracy: 0.6027\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.5206 - categorical_accuracy: 0.7622 - val_loss: 1.9805 - val_categorical_accuracy: 0.6497\n",
            "Epoch 17/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.4818 - categorical_accuracy: 0.7718 - val_loss: 1.9364 - val_categorical_accuracy: 0.6587\n",
            "Epoch 18/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.4403 - categorical_accuracy: 0.7736 - val_loss: 1.9243 - val_categorical_accuracy: 0.6593\n",
            "Epoch 19/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.4163 - categorical_accuracy: 0.7819 - val_loss: 1.7481 - val_categorical_accuracy: 0.6971\n",
            "Epoch 20/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.3741 - categorical_accuracy: 0.7878 - val_loss: 1.7688 - val_categorical_accuracy: 0.6988\n",
            "Epoch 21/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.3287 - categorical_accuracy: 0.7966 - val_loss: 1.7027 - val_categorical_accuracy: 0.7112\n",
            "Epoch 22/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.3244 - categorical_accuracy: 0.8030 - val_loss: 1.7950 - val_categorical_accuracy: 0.6969\n",
            "Epoch 23/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.2684 - categorical_accuracy: 0.8068 - val_loss: 1.5997 - val_categorical_accuracy: 0.7313\n",
            "Epoch 24/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.2689 - categorical_accuracy: 0.8108 - val_loss: 1.6615 - val_categorical_accuracy: 0.7117\n",
            "Epoch 25/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.2419 - categorical_accuracy: 0.8140 - val_loss: 1.6273 - val_categorical_accuracy: 0.7147\n",
            "Epoch 26/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.2142 - categorical_accuracy: 0.8243 - val_loss: 1.6502 - val_categorical_accuracy: 0.7178\n",
            "Epoch 27/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.2051 - categorical_accuracy: 0.8253 - val_loss: 1.4510 - val_categorical_accuracy: 0.7501\n",
            "Epoch 28/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.1761 - categorical_accuracy: 0.8294 - val_loss: 1.4532 - val_categorical_accuracy: 0.7585\n",
            "Epoch 29/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.1482 - categorical_accuracy: 0.8366 - val_loss: 1.6568 - val_categorical_accuracy: 0.7297\n",
            "Epoch 30/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.1528 - categorical_accuracy: 0.8361 - val_loss: 1.5009 - val_categorical_accuracy: 0.7474\n",
            "Epoch 31/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.1429 - categorical_accuracy: 0.8364 - val_loss: 1.5112 - val_categorical_accuracy: 0.7511\n",
            "Epoch 32/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.1251 - categorical_accuracy: 0.8401 - val_loss: 1.4779 - val_categorical_accuracy: 0.7609\n",
            "Epoch 33/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.1100 - categorical_accuracy: 0.8450 - val_loss: 1.5169 - val_categorical_accuracy: 0.7569\n",
            "Epoch 34/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.0930 - categorical_accuracy: 0.8505 - val_loss: 1.4466 - val_categorical_accuracy: 0.7683\n",
            "Epoch 35/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0802 - categorical_accuracy: 0.8508 - val_loss: 1.3966 - val_categorical_accuracy: 0.7818\n",
            "Epoch 36/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0853 - categorical_accuracy: 0.8513 - val_loss: 1.4932 - val_categorical_accuracy: 0.7564\n",
            "Epoch 37/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0553 - categorical_accuracy: 0.8536 - val_loss: 1.4369 - val_categorical_accuracy: 0.7746\n",
            "Epoch 38/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0487 - categorical_accuracy: 0.8581 - val_loss: 1.5618 - val_categorical_accuracy: 0.7503\n",
            "Epoch 39/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0544 - categorical_accuracy: 0.8577 - val_loss: 1.4561 - val_categorical_accuracy: 0.7742\n",
            "Epoch 40/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0467 - categorical_accuracy: 0.8614 - val_loss: 1.3661 - val_categorical_accuracy: 0.7812\n",
            "Epoch 41/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0122 - categorical_accuracy: 0.8647 - val_loss: 1.5246 - val_categorical_accuracy: 0.7593\n",
            "Epoch 42/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0172 - categorical_accuracy: 0.8624 - val_loss: 1.4805 - val_categorical_accuracy: 0.7773\n",
            "Epoch 43/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 1.0418 - categorical_accuracy: 0.8596 - val_loss: 1.3592 - val_categorical_accuracy: 0.7990\n",
            "Epoch 44/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0054 - categorical_accuracy: 0.8641 - val_loss: 1.5083 - val_categorical_accuracy: 0.7791\n",
            "Epoch 45/50\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.9905 - categorical_accuracy: 0.8706 - val_loss: 1.3072 - val_categorical_accuracy: 0.8063\n",
            "Epoch 46/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.9998 - categorical_accuracy: 0.8673 - val_loss: 1.4550 - val_categorical_accuracy: 0.7726\n",
            "Epoch 47/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 1.0000 - categorical_accuracy: 0.8692 - val_loss: 1.3014 - val_categorical_accuracy: 0.7994\n",
            "Epoch 48/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.9721 - categorical_accuracy: 0.8742 - val_loss: 1.2932 - val_categorical_accuracy: 0.8063\n",
            "Epoch 49/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.9562 - categorical_accuracy: 0.8798 - val_loss: 1.2643 - val_categorical_accuracy: 0.8049\n",
            "Epoch 50/50\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.9675 - categorical_accuracy: 0.8758 - val_loss: 1.4238 - val_categorical_accuracy: 0.7910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huOy23jiKHH2",
        "outputId": "63e9e401-7d79-4141-ad62-f6a8d01e5afe"
      },
      "source": [
        "model_wm_01_dist = load_best_checkpoint(\n",
        "    model_wm_01_dist, \n",
        "    get_model_name(0.1, distillation=True))\n",
        "model_wm_01_dist.evaluate(x_test, y_dist_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "153/153 [==============================] - 0s 3ms/step - loss: 1.3072 - categorical_accuracy: 0.8063\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3071538209915161, 0.80633944272995]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YImA3TqpM7oG"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "height": 279
        },
        "id": "s2HPwGutNHO-",
        "outputId": "ec6182b1-050a-438a-a06e-795c9ff4a297"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "plt.plot(model_wm_01_history['val_categorical_accuracy'],\n",
        "         label='test accuracy baseline',\n",
        "         c='orange',ls='--')\n",
        "\n",
        "plt.plot(model_wm_01_dist_history['val_categorical_accuracy'],\n",
        "         label='test accuracy distilled',\n",
        "         c='dodgerblue', ls='-')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}