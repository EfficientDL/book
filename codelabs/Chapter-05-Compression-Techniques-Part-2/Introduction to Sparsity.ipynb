{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import gzip\n",
    "import operator, random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity improves Model Compression\n",
    "We use a randomly generated sample for this demonstration. We evaluate and compare the compression ratios achieved without and with sparsification. The expected outcome is higher compression ratio for the sparse model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Size: 40000\n",
      "Original Compressed Size: 35928\n",
      "Sparsified Compressed Size: 23944\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.random(size=(100, 100)).astype(np.float32)\n",
    "sparsity_rate = 0.4 # The percentage of weights that are zeroed out.\n",
    "\n",
    "# Sparsify the weights by setting a fraction of the weights to zero.\n",
    "def sparsify_smallest(w, sr):\n",
    "    w = w.copy()\n",
    "    w_1d = np.reshape(w, -1)\n",
    "    w_1d_sorted = np.sort(w_1d)\n",
    "    threshold = w_1d_sorted[int(w_1d_sorted.size * sr)]\n",
    "\n",
    "    # Set the weights to zero if they are less than the threshold.\n",
    "    w[w < threshold] = 0\n",
    "\n",
    "    return w\n",
    "\n",
    "def compress_and_save(w):\n",
    "    # Compress the weights matrix using gzip.\n",
    "    compressed_w = gzip.compress(w.tobytes())\n",
    "    return compressed_w\n",
    "\n",
    "sparse_weights = sparsify_smallest(weights, sparsity_rate)\n",
    "\n",
    "print('Original Size:', reduce(operator.mul, weights.shape)*weights.itemsize)\n",
    "\n",
    "weights_compressed = compress_and_save(weights)\n",
    "print('Original Compressed Size:', len(weights_compressed))\n",
    "\n",
    "weights_sparsified_compressed = compress_and_save(sparse_weights)\n",
    "print('Sparsified Compressed Size:', len(weights_sparsified_compressed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity induced compression in a logic gate classifier\n",
    "This exercise is a step up from the previous example which used non-trainable weights. In this example, we will create a AND and OR gate datasets with labels -1 and 1 respectively. We will train a simple classifier to predict AND and OR gates. Next, we will conduct another round of training with induced sparsity in each training step. The expectation is that both the training procedures converge to comparable losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate AND and OR gate dataset\n",
    "The AND samples are labeled -1 and the OR samples get 1 label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(count=100):\n",
    "    def and_fn():\n",
    "        samples = [((0, 0, 0), 1), ((0, 1, 0), 1), ((1, 0, 0), 1), ((1, 1, 1), 1)]\n",
    "        return random.choice(samples)\n",
    "    \n",
    "    def or_fn():\n",
    "        samples = [((0, 0, 0), -1), ((0, 1, 1), -1), ((1, 0, 1), -1), ((1, 1, 1), -1)]\n",
    "        return random.choice(samples)\n",
    "\n",
    "    def make_sample():\n",
    "        sample_fn = and_fn if np.random.random() < 0.5 else or_fn\n",
    "        return sample_fn()\n",
    "\n",
    "    dataset = list(map(lambda _:  make_sample(), range(count)))\n",
    "    return dataset\n",
    "\n",
    "dataset = make_dataset()\n",
    "X = np.array([x[0] for x in dataset]).astype(np.float32)\n",
    "Y = np.array([x[1] for x in dataset]).astype(np.float32)\n",
    "\n",
    "and_test_samples = tf.constant(np.array([(0, 0, 0), (0, 1, 0), (1, 0, 0), (1, 1, 1)]).astype('float32'))\n",
    "# X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Training\n",
      "--------------------------------\n",
      "Step: 0  Loss: 2.3999939\n",
      "Step: 1  Loss: 2.39999366\n",
      "Step: 2  Loss: 2.39999342\n",
      "Step: 3  Loss: 2.39999318\n",
      "Step: 4  Loss: 2.39999294\n",
      "Step: 5  Loss: 2.39999247\n",
      "Step: 6  Loss: 2.39999199\n",
      "Step: 7  Loss: 2.39999127\n",
      "Step: 8  Loss: 2.39999104\n",
      "Step: 9  Loss: 2.39999056\n",
      "Step: 10  Loss: 2.39999\n",
      "Step: 11  Loss: 2.3999896\n",
      "Step: 12  Loss: 2.39998889\n",
      "Step: 13  Loss: 2.39998817\n",
      "Step: 14  Loss: 2.39998794\n",
      "Step: 15  Loss: 2.39998698\n",
      "Step: 16  Loss: 2.39998627\n",
      "Step: 17  Loss: 2.39998555\n",
      "Step: 18  Loss: 2.3999846\n",
      "Step: 19  Loss: 2.39998364\n",
      "Step: 20  Loss: 2.39998245\n",
      "Step: 21  Loss: 2.39998174\n",
      "Step: 22  Loss: 2.39998031\n",
      "Step: 23  Loss: 2.39997935\n",
      "Step: 24  Loss: 2.39997816\n",
      "Step: 25  Loss: 2.39997649\n",
      "Step: 26  Loss: 2.39997482\n",
      "Step: 27  Loss: 2.39997339\n",
      "Step: 28  Loss: 2.39997172\n",
      "Step: 29  Loss: 2.39996958\n",
      "Step: 30  Loss: 2.39996743\n",
      "Step: 31  Loss: 2.39996529\n",
      "Step: 32  Loss: 2.3999629\n",
      "Step: 33  Loss: 2.3999598\n",
      "Step: 34  Loss: 2.39995694\n",
      "Step: 35  Loss: 2.39995384\n",
      "Step: 36  Loss: 2.39995027\n",
      "Step: 37  Loss: 2.39994669\n",
      "Step: 38  Loss: 2.39994264\n",
      "Step: 39  Loss: 2.39993811\n",
      "Step: 40  Loss: 2.3999331\n",
      "Step: 41  Loss: 2.39992762\n",
      "Step: 42  Loss: 2.39992261\n",
      "Step: 43  Loss: 2.39991713\n",
      "Step: 44  Loss: 2.39991117\n",
      "Step: 45  Loss: 2.39990473\n",
      "Step: 46  Loss: 2.39989805\n",
      "Step: 47  Loss: 2.39989018\n",
      "Step: 48  Loss: 2.39988208\n",
      "Step: 49  Loss: 2.39987302\n",
      "Step: 50  Loss: 2.39986324\n",
      "Step: 51  Loss: 2.39985275\n",
      "Step: 52  Loss: 2.39984131\n",
      "Step: 53  Loss: 2.39982891\n",
      "Step: 54  Loss: 2.39981508\n",
      "Step: 55  Loss: 2.3998003\n",
      "Step: 56  Loss: 2.39978433\n",
      "Step: 57  Loss: 2.39976668\n",
      "Step: 58  Loss: 2.39974737\n",
      "Step: 59  Loss: 2.39972639\n",
      "Step: 60  Loss: 2.39970326\n",
      "Step: 61  Loss: 2.39967799\n",
      "Step: 62  Loss: 2.39965081\n",
      "Step: 63  Loss: 2.39962053\n",
      "Step: 64  Loss: 2.39958787\n",
      "Step: 65  Loss: 2.39955139\n",
      "Step: 66  Loss: 2.39951158\n",
      "Step: 67  Loss: 2.39946818\n",
      "Step: 68  Loss: 2.39942026\n",
      "Step: 69  Loss: 2.39936757\n",
      "Step: 70  Loss: 2.39930964\n",
      "Step: 71  Loss: 2.39924574\n",
      "Step: 72  Loss: 2.39917541\n",
      "Step: 73  Loss: 2.39909768\n",
      "Step: 74  Loss: 2.39901209\n",
      "Step: 75  Loss: 2.39891744\n",
      "Step: 76  Loss: 2.39881253\n",
      "Step: 77  Loss: 2.39869642\n",
      "Step: 78  Loss: 2.39856744\n",
      "Step: 79  Loss: 2.39842415\n",
      "Step: 80  Loss: 2.39826441\n",
      "Step: 81  Loss: 2.39808655\n",
      "Step: 82  Loss: 2.39788795\n",
      "Step: 83  Loss: 2.3976655\n",
      "Step: 84  Loss: 2.39741588\n",
      "Step: 85  Loss: 2.39713454\n",
      "Step: 86  Loss: 2.39681673\n",
      "Step: 87  Loss: 2.39645672\n",
      "Step: 88  Loss: 2.39604712\n",
      "Step: 89  Loss: 2.3955791\n",
      "Step: 90  Loss: 2.3950417\n",
      "Step: 91  Loss: 2.39442182\n",
      "Step: 92  Loss: 2.39370298\n",
      "Step: 93  Loss: 2.39286399\n",
      "Step: 94  Loss: 2.39187932\n",
      "Step: 95  Loss: 2.39071584\n",
      "Step: 96  Loss: 2.38933325\n",
      "Step: 97  Loss: 2.38767958\n",
      "Step: 98  Loss: 2.3856895\n",
      "Step: 99  Loss: 2.38328052\n",
      "Step: 100  Loss: 2.38034987\n",
      "Step: 101  Loss: 2.37676978\n",
      "Step: 102  Loss: 2.3723824\n",
      "Step: 103  Loss: 2.36699796\n",
      "Step: 104  Loss: 2.36039257\n",
      "Step: 105  Loss: 2.35231137\n",
      "Step: 106  Loss: 2.34248209\n",
      "Step: 107  Loss: 2.33064127\n",
      "Step: 108  Loss: 2.31658244\n",
      "Step: 109  Loss: 2.30023122\n",
      "Step: 110  Loss: 2.28174782\n",
      "Step: 111  Loss: 2.26163983\n",
      "Step: 112  Loss: 2.24060512\n",
      "Step: 113  Loss: 2.22015381\n",
      "Step: 114  Loss: 2.20208716\n",
      "Step: 115  Loss: 2.18794966\n",
      "Step: 116  Loss: 2.17831087\n",
      "Step: 117  Loss: 2.17246866\n",
      "Step: 118  Loss: 2.16888165\n",
      "Step: 119  Loss: 2.16598034\n",
      "Step: 120  Loss: 2.16267967\n",
      "Step: 121  Loss: 2.15840101\n",
      "Step: 122  Loss: 2.15289259\n",
      "Step: 123  Loss: 2.14605784\n",
      "Step: 124  Loss: 2.13747859\n",
      "Step: 125  Loss: 2.12740421\n",
      "Step: 126  Loss: 2.11563349\n",
      "Step: 127  Loss: 2.10209846\n",
      "Step: 128  Loss: 2.08668756\n",
      "Step: 129  Loss: 2.06926012\n",
      "Step: 130  Loss: 2.04965687\n",
      "Step: 131  Loss: 2.02771139\n",
      "Step: 132  Loss: 2.0034132\n",
      "Step: 133  Loss: 1.97653437\n",
      "Step: 134  Loss: 1.94696963\n",
      "Step: 135  Loss: 1.91483295\n",
      "Step: 136  Loss: 1.88045955\n",
      "Step: 137  Loss: 1.84448469\n",
      "Step: 138  Loss: 1.8077451\n",
      "Step: 139  Loss: 1.77119505\n",
      "Step: 140  Loss: 1.73556244\n",
      "Step: 141  Loss: 1.70105815\n",
      "Step: 142  Loss: 1.66722143\n",
      "Step: 143  Loss: 1.63263738\n",
      "Step: 144  Loss: 1.59563351\n",
      "Step: 145  Loss: 1.55550599\n",
      "Step: 146  Loss: 1.51413155\n",
      "Step: 147  Loss: 1.47695446\n",
      "Step: 148  Loss: 1.45166874\n",
      "Step: 149  Loss: 1.44344425\n",
      "Step: 150  Loss: 1.45018351\n",
      "Step: 151  Loss: 1.46428645\n",
      "Step: 152  Loss: 1.47885978\n",
      "Step: 153  Loss: 1.49045217\n",
      "Step: 154  Loss: 1.49807787\n",
      "Step: 155  Loss: 1.5017947\n",
      "Step: 156  Loss: 1.50195622\n",
      "Step: 157  Loss: 1.4989562\n",
      "Step: 158  Loss: 1.49319232\n",
      "Step: 159  Loss: 1.48513067\n",
      "Step: 160  Loss: 1.47539604\n",
      "Step: 161  Loss: 1.46485853\n",
      "Step: 162  Loss: 1.45464444\n",
      "Step: 163  Loss: 1.44600081\n",
      "Step: 164  Loss: 1.43997645\n",
      "Step: 165  Loss: 1.43702519\n",
      "Step: 166  Loss: 1.4367801\n",
      "Step: 167  Loss: 1.43819237\n",
      "Step: 168  Loss: 1.43997514\n",
      "Step: 169  Loss: 1.44104385\n",
      "Step: 170  Loss: 1.44074726\n",
      "Step: 171  Loss: 1.43889141\n",
      "Step: 172  Loss: 1.43565\n",
      "Step: 173  Loss: 1.43145394\n",
      "Step: 174  Loss: 1.42687619\n",
      "Step: 175  Loss: 1.42247581\n",
      "Step: 176  Loss: 1.41872323\n",
      "Step: 177  Loss: 1.41588604\n",
      "Step: 178  Loss: 1.41398728\n",
      "Step: 179  Loss: 1.41285598\n",
      "Step: 180  Loss: 1.4122107\n",
      "Step: 181  Loss: 1.41175258\n",
      "Step: 182  Loss: 1.41123986\n",
      "Step: 183  Loss: 1.41052699\n",
      "Step: 184  Loss: 1.40957141\n",
      "Step: 185  Loss: 1.40841758\n",
      "Step: 186  Loss: 1.40717125\n",
      "Step: 187  Loss: 1.40596724\n",
      "Step: 188  Loss: 1.40493131\n",
      "Step: 189  Loss: 1.40415657\n",
      "Step: 190  Loss: 1.40368664\n",
      "Step: 191  Loss: 1.4035002\n",
      "Step: 192  Loss: 1.40352917\n",
      "Step: 193  Loss: 1.40367496\n",
      "Step: 194  Loss: 1.40383732\n",
      "Step: 195  Loss: 1.40394056\n",
      "Step: 196  Loss: 1.40393507\n",
      "Step: 197  Loss: 1.40382218\n",
      "Step: 198  Loss: 1.40363169\n",
      "Step: 199  Loss: 1.40340793\n",
      "Step: 200  Loss: 1.40319848\n",
      "Step: 201  Loss: 1.40303886\n",
      "Step: 202  Loss: 1.40294611\n",
      "Step: 203  Loss: 1.40291715\n",
      "Step: 204  Loss: 1.40293479\n",
      "Step: 205  Loss: 1.40297413\n",
      "Step: 206  Loss: 1.40300977\n",
      "Step: 207  Loss: 1.40302145\n",
      "Step: 208  Loss: 1.40299845\n",
      "Step: 209  Loss: 1.40294099\n",
      "Step: 210  Loss: 1.40285754\n",
      "Step: 211  Loss: 1.4027617\n",
      "Step: 212  Loss: 1.40266776\n",
      "Step: 213  Loss: 1.40258849\n",
      "Step: 214  Loss: 1.40253055\n",
      "Step: 215  Loss: 1.40249527\n",
      "Step: 216  Loss: 1.40247869\n",
      "Step: 217  Loss: 1.40247357\n",
      "Step: 218  Loss: 1.40247178\n",
      "Step: 219  Loss: 1.40246964\n",
      "Step: 220  Loss: 1.40245712\n",
      "Step: 221  Loss: 1.40243506\n",
      "Step: 222  Loss: 1.40240872\n",
      "Step: 223  Loss: 1.40237927\n",
      "Step: 224  Loss: 1.40235114\n",
      "Step: 225  Loss: 1.40232754\n",
      "Step: 226  Loss: 1.40231013\n",
      "Step: 227  Loss: 1.4023006\n",
      "Step: 228  Loss: 1.40229452\n",
      "Step: 229  Loss: 1.40229106\n",
      "Step: 230  Loss: 1.4022882\n",
      "Step: 231  Loss: 1.40228271\n",
      "Step: 232  Loss: 1.40227354\n",
      "Step: 233  Loss: 1.4022609\n",
      "Step: 234  Loss: 1.40224564\n",
      "Step: 235  Loss: 1.40223074\n",
      "Step: 236  Loss: 1.40221477\n",
      "Step: 237  Loss: 1.40220046\n",
      "Step: 238  Loss: 1.40218866\n",
      "Step: 239  Loss: 1.40217829\n",
      "Step: 240  Loss: 1.40216875\n",
      "Step: 241  Loss: 1.40215933\n",
      "Step: 242  Loss: 1.40214956\n",
      "Step: 243  Loss: 1.40214217\n",
      "Step: 244  Loss: 1.4021318\n",
      "Step: 245  Loss: 1.40211916\n",
      "Step: 246  Loss: 1.40210783\n",
      "Step: 247  Loss: 1.40209639\n",
      "Step: 248  Loss: 1.40208507\n",
      "Step: 249  Loss: 1.40207422\n",
      "Step: 250  Loss: 1.40206516\n",
      "Step: 251  Loss: 1.40205538\n",
      "Step: 252  Loss: 1.4020474\n",
      "Step: 253  Loss: 1.40203941\n",
      "Step: 254  Loss: 1.40203083\n",
      "Step: 255  Loss: 1.40202141\n",
      "Step: 256  Loss: 1.40201128\n",
      "Step: 257  Loss: 1.40200067\n",
      "Step: 258  Loss: 1.40199029\n",
      "Step: 259  Loss: 1.40198028\n",
      "Step: 260  Loss: 1.40197062\n",
      "Step: 261  Loss: 1.40196145\n",
      "Step: 262  Loss: 1.40195191\n",
      "Step: 263  Loss: 1.40194201\n",
      "Step: 264  Loss: 1.40193164\n",
      "Step: 265  Loss: 1.40192163\n",
      "Step: 266  Loss: 1.40191114\n",
      "Step: 267  Loss: 1.40190148\n",
      "Step: 268  Loss: 1.40189159\n",
      "Step: 269  Loss: 1.40188098\n",
      "Step: 270  Loss: 1.40187025\n",
      "Step: 271  Loss: 1.40185905\n",
      "Step: 272  Loss: 1.40184748\n",
      "Step: 273  Loss: 1.40183878\n",
      "Step: 274  Loss: 1.40182817\n",
      "Step: 275  Loss: 1.40181506\n",
      "Step: 276  Loss: 1.40180564\n",
      "Step: 277  Loss: 1.40179586\n",
      "Step: 278  Loss: 1.40178537\n",
      "Step: 279  Loss: 1.40177441\n",
      "Step: 280  Loss: 1.40176296\n",
      "Step: 281  Loss: 1.40175116\n",
      "Step: 282  Loss: 1.40173864\n",
      "Step: 283  Loss: 1.40172601\n",
      "Step: 284  Loss: 1.40171397\n",
      "Step: 285  Loss: 1.40170288\n",
      "Step: 286  Loss: 1.40169\n",
      "Step: 287  Loss: 1.40167844\n",
      "Step: 288  Loss: 1.4016664\n",
      "Step: 289  Loss: 1.40165472\n",
      "Step: 290  Loss: 1.40164208\n",
      "Step: 291  Loss: 1.40162981\n",
      "Step: 292  Loss: 1.40161729\n",
      "Step: 293  Loss: 1.40160525\n",
      "Step: 294  Loss: 1.40159297\n",
      "Step: 295  Loss: 1.4015801\n",
      "Step: 296  Loss: 1.4015671\n",
      "Step: 297  Loss: 1.40155435\n",
      "Step: 298  Loss: 1.40154123\n",
      "Step: 299  Loss: 1.401528\n",
      "Step: 300  Loss: 1.40151525\n",
      "Step: 301  Loss: 1.40150201\n",
      "Step: 302  Loss: 1.4014883\n",
      "Step: 303  Loss: 1.40147531\n",
      "Step: 304  Loss: 1.40146089\n",
      "Step: 305  Loss: 1.40144706\n",
      "Step: 306  Loss: 1.40143418\n",
      "Step: 307  Loss: 1.40141916\n",
      "Step: 308  Loss: 1.4014051\n",
      "Step: 309  Loss: 1.40139103\n",
      "Step: 310  Loss: 1.40137696\n",
      "Step: 311  Loss: 1.40136266\n",
      "Step: 312  Loss: 1.40134788\n",
      "Step: 313  Loss: 1.40133286\n",
      "Step: 314  Loss: 1.40131772\n",
      "Step: 315  Loss: 1.4013027\n",
      "Step: 316  Loss: 1.40128815\n",
      "Step: 317  Loss: 1.40127218\n",
      "Step: 318  Loss: 1.40125704\n",
      "Step: 319  Loss: 1.4012413\n",
      "Step: 320  Loss: 1.40122592\n",
      "Step: 321  Loss: 1.40121031\n",
      "Step: 322  Loss: 1.40119398\n",
      "Step: 323  Loss: 1.401178\n",
      "Step: 324  Loss: 1.40116107\n",
      "Step: 325  Loss: 1.4011445\n",
      "Step: 326  Loss: 1.40112901\n",
      "Step: 327  Loss: 1.40111256\n",
      "Step: 328  Loss: 1.40109563\n",
      "Step: 329  Loss: 1.40107775\n",
      "Step: 330  Loss: 1.40106106\n",
      "Step: 331  Loss: 1.40104318\n",
      "Step: 332  Loss: 1.40102589\n",
      "Step: 333  Loss: 1.40100873\n",
      "Step: 334  Loss: 1.40099096\n",
      "Step: 335  Loss: 1.40097237\n",
      "Step: 336  Loss: 1.40095294\n",
      "Step: 337  Loss: 1.40093684\n",
      "Step: 338  Loss: 1.40091848\n",
      "Step: 339  Loss: 1.40089703\n",
      "Step: 340  Loss: 1.40088\n",
      "Step: 341  Loss: 1.40086222\n",
      "Step: 342  Loss: 1.40084362\n",
      "Step: 343  Loss: 1.40082407\n",
      "Step: 344  Loss: 1.40080369\n",
      "Step: 345  Loss: 1.40078282\n",
      "Step: 346  Loss: 1.40076089\n",
      "Step: 347  Loss: 1.40074062\n",
      "Step: 348  Loss: 1.40072119\n",
      "Step: 349  Loss: 1.40069842\n",
      "Step: 350  Loss: 1.40067816\n",
      "Step: 351  Loss: 1.40065813\n",
      "Step: 352  Loss: 1.40063703\n",
      "Step: 353  Loss: 1.4006151\n",
      "Step: 354  Loss: 1.40059233\n",
      "Step: 355  Loss: 1.40056837\n",
      "Step: 356  Loss: 1.40054703\n",
      "Step: 357  Loss: 1.40052521\n",
      "Step: 358  Loss: 1.4005\n",
      "Step: 359  Loss: 1.40047824\n",
      "Step: 360  Loss: 1.40045607\n",
      "Step: 361  Loss: 1.40043294\n",
      "Step: 362  Loss: 1.40040886\n",
      "Step: 363  Loss: 1.40038359\n",
      "Step: 364  Loss: 1.4003576\n",
      "Step: 365  Loss: 1.40033209\n",
      "Step: 366  Loss: 1.40030777\n",
      "Step: 367  Loss: 1.40028059\n",
      "Step: 368  Loss: 1.40025508\n",
      "Step: 369  Loss: 1.40022886\n",
      "Step: 370  Loss: 1.40020311\n",
      "Step: 371  Loss: 1.40017676\n",
      "Step: 372  Loss: 1.40014911\n",
      "Step: 373  Loss: 1.400123\n",
      "Step: 374  Loss: 1.40009475\n",
      "Step: 375  Loss: 1.40006673\n",
      "Step: 376  Loss: 1.40003943\n",
      "Step: 377  Loss: 1.40001059\n",
      "Step: 378  Loss: 1.39998078\n",
      "Step: 379  Loss: 1.3999511\n",
      "Step: 380  Loss: 1.39992106\n",
      "Step: 381  Loss: 1.39989173\n",
      "Step: 382  Loss: 1.39986205\n",
      "Step: 383  Loss: 1.39983094\n",
      "Step: 384  Loss: 1.39979851\n",
      "Step: 385  Loss: 1.39976788\n",
      "Step: 386  Loss: 1.39973581\n",
      "Step: 387  Loss: 1.39970195\n",
      "Step: 388  Loss: 1.39966965\n",
      "Step: 389  Loss: 1.39963591\n",
      "Step: 390  Loss: 1.39960194\n",
      "Step: 391  Loss: 1.39956701\n",
      "Step: 392  Loss: 1.39953256\n",
      "Step: 393  Loss: 1.39949787\n",
      "Step: 394  Loss: 1.39946246\n",
      "Step: 395  Loss: 1.39942551\n",
      "Step: 396  Loss: 1.39939058\n",
      "Step: 397  Loss: 1.39935291\n",
      "Step: 398  Loss: 1.39931464\n",
      "Step: 399  Loss: 1.39927721\n",
      "Step: 400  Loss: 1.39923811\n",
      "Step: 401  Loss: 1.39919734\n",
      "Step: 402  Loss: 1.39916086\n",
      "Step: 403  Loss: 1.39912081\n",
      "Step: 404  Loss: 1.39907658\n",
      "Step: 405  Loss: 1.39903736\n",
      "Step: 406  Loss: 1.39899731\n",
      "Step: 407  Loss: 1.39895523\n",
      "Step: 408  Loss: 1.39891112\n",
      "Step: 409  Loss: 1.3988651\n",
      "Step: 410  Loss: 1.3988173\n",
      "Step: 411  Loss: 1.39877594\n",
      "Step: 412  Loss: 1.39873207\n",
      "Step: 413  Loss: 1.39868355\n",
      "Step: 414  Loss: 1.39863086\n",
      "Step: 415  Loss: 1.39858651\n",
      "Step: 416  Loss: 1.39854062\n",
      "Step: 417  Loss: 1.3984921\n",
      "Step: 418  Loss: 1.39844131\n",
      "Step: 419  Loss: 1.39838803\n",
      "Step: 420  Loss: 1.39833283\n",
      "Step: 421  Loss: 1.39827538\n",
      "Step: 422  Loss: 1.39822829\n",
      "Step: 423  Loss: 1.39817691\n",
      "Step: 424  Loss: 1.39812052\n",
      "Step: 425  Loss: 1.39805937\n",
      "Step: 426  Loss: 1.39799547\n",
      "Step: 427  Loss: 1.39793944\n",
      "Step: 428  Loss: 1.39788043\n",
      "Step: 429  Loss: 1.39781857\n",
      "Step: 430  Loss: 1.39775372\n",
      "Step: 431  Loss: 1.39769506\n",
      "Step: 432  Loss: 1.39763236\n",
      "Step: 433  Loss: 1.39756465\n",
      "Step: 434  Loss: 1.39749467\n",
      "Step: 435  Loss: 1.39742863\n",
      "Step: 436  Loss: 1.39735949\n",
      "Step: 437  Loss: 1.39728665\n",
      "Step: 438  Loss: 1.39721894\n",
      "Step: 439  Loss: 1.39714682\n",
      "Step: 440  Loss: 1.39706933\n",
      "Step: 441  Loss: 1.3969934\n",
      "Step: 442  Loss: 1.39691818\n",
      "Step: 443  Loss: 1.39684343\n",
      "Step: 444  Loss: 1.39676642\n",
      "Step: 445  Loss: 1.39669061\n",
      "Step: 446  Loss: 1.39661419\n",
      "Step: 447  Loss: 1.39653218\n",
      "Step: 448  Loss: 1.39645374\n",
      "Step: 449  Loss: 1.39637315\n",
      "Step: 450  Loss: 1.39628804\n",
      "Step: 451  Loss: 1.39619863\n",
      "Step: 452  Loss: 1.3961134\n",
      "Step: 453  Loss: 1.3960259\n",
      "Step: 454  Loss: 1.39593244\n",
      "Step: 455  Loss: 1.39583385\n",
      "Step: 456  Loss: 1.39573944\n",
      "Step: 457  Loss: 1.39564037\n",
      "Step: 458  Loss: 1.39554155\n",
      "Step: 459  Loss: 1.39543819\n",
      "Step: 460  Loss: 1.39533508\n",
      "Step: 461  Loss: 1.39522672\n",
      "Step: 462  Loss: 1.39512181\n",
      "Step: 463  Loss: 1.39501071\n",
      "Step: 464  Loss: 1.39489329\n",
      "Step: 465  Loss: 1.39477682\n",
      "Step: 466  Loss: 1.39465845\n",
      "Step: 467  Loss: 1.39453316\n",
      "Step: 468  Loss: 1.39440644\n",
      "Step: 469  Loss: 1.39427841\n",
      "Step: 470  Loss: 1.3941437\n",
      "Step: 471  Loss: 1.39400876\n",
      "Step: 472  Loss: 1.39386964\n",
      "Step: 473  Loss: 1.39372241\n",
      "Step: 474  Loss: 1.39358115\n",
      "Step: 475  Loss: 1.3934325\n",
      "Step: 476  Loss: 1.3932761\n",
      "Step: 477  Loss: 1.39311242\n",
      "Step: 478  Loss: 1.39294291\n",
      "Step: 479  Loss: 1.3927747\n",
      "Step: 480  Loss: 1.39259875\n",
      "Step: 481  Loss: 1.39241838\n",
      "Step: 482  Loss: 1.39223361\n",
      "Step: 483  Loss: 1.39203978\n",
      "Step: 484  Loss: 1.3918426\n",
      "Step: 485  Loss: 1.39163947\n",
      "Step: 486  Loss: 1.39142835\n",
      "Step: 487  Loss: 1.39120781\n",
      "Step: 488  Loss: 1.39098442\n",
      "Step: 489  Loss: 1.39074802\n",
      "Step: 490  Loss: 1.39050603\n",
      "Step: 491  Loss: 1.39025605\n",
      "Step: 492  Loss: 1.38999426\n",
      "Step: 493  Loss: 1.38972068\n",
      "Step: 494  Loss: 1.38943529\n",
      "Step: 495  Loss: 1.38913882\n",
      "Step: 496  Loss: 1.38883078\n",
      "Step: 497  Loss: 1.38850987\n",
      "Step: 498  Loss: 1.38817561\n",
      "Step: 499  Loss: 1.38782609\n",
      "Sparsified Training\n",
      "--------------------------------\n",
      "Step: 0  Loss: 2.3999939\n",
      "Step: 1  Loss: 2.35252953\n",
      "Step: 2  Loss: 2.32367611\n",
      "Step: 3  Loss: 2.32237172\n",
      "Step: 4  Loss: 2.32095551\n",
      "Step: 5  Loss: 2.31946039\n",
      "Step: 6  Loss: 2.3179028\n",
      "Step: 7  Loss: 2.31629062\n",
      "Step: 8  Loss: 2.31462789\n",
      "Step: 9  Loss: 2.31291723\n",
      "Step: 10  Loss: 2.31116\n",
      "Step: 11  Loss: 2.30935693\n",
      "Step: 12  Loss: 2.30750823\n",
      "Step: 13  Loss: 2.3056128\n",
      "Step: 14  Loss: 2.30367064\n",
      "Step: 15  Loss: 2.3016808\n",
      "Step: 16  Loss: 2.29964185\n",
      "Step: 17  Loss: 2.29755354\n",
      "Step: 18  Loss: 2.29541373\n",
      "Step: 19  Loss: 2.29322124\n",
      "Step: 20  Loss: 2.29097533\n",
      "Step: 21  Loss: 2.28867364\n",
      "Step: 22  Loss: 2.2863152\n",
      "Step: 23  Loss: 2.28389788\n",
      "Step: 24  Loss: 2.28142071\n",
      "Step: 25  Loss: 2.27888083\n",
      "Step: 26  Loss: 2.27627778\n",
      "Step: 27  Loss: 2.27360821\n",
      "Step: 28  Loss: 2.27087116\n",
      "Step: 29  Loss: 2.2680645\n",
      "Step: 30  Loss: 2.26518583\n",
      "Step: 31  Loss: 2.26223326\n",
      "Step: 32  Loss: 2.25920463\n",
      "Step: 33  Loss: 2.25609803\n",
      "Step: 34  Loss: 2.25291085\n",
      "Step: 35  Loss: 2.24964142\n",
      "Step: 36  Loss: 2.24628615\n",
      "Step: 37  Loss: 2.24284363\n",
      "Step: 38  Loss: 2.23931217\n",
      "Step: 39  Loss: 2.23568749\n",
      "Step: 40  Loss: 2.23196816\n",
      "Step: 41  Loss: 2.2281518\n",
      "Step: 42  Loss: 2.22423506\n",
      "Step: 43  Loss: 2.22021627\n",
      "Step: 44  Loss: 2.21609259\n",
      "Step: 45  Loss: 2.2118609\n",
      "Step: 46  Loss: 2.20751858\n",
      "Step: 47  Loss: 2.20306325\n",
      "Step: 48  Loss: 2.19849181\n",
      "Step: 49  Loss: 2.19380212\n",
      "Step: 50  Loss: 2.18899107\n",
      "Step: 51  Loss: 2.18405533\n",
      "Step: 52  Loss: 2.17899323\n",
      "Step: 53  Loss: 2.17380142\n",
      "Step: 54  Loss: 2.16847706\n",
      "Step: 55  Loss: 2.16301799\n",
      "Step: 56  Loss: 2.15742064\n",
      "Step: 57  Loss: 2.15168309\n",
      "Step: 58  Loss: 2.14580226\n",
      "Step: 59  Loss: 2.13977599\n",
      "Step: 60  Loss: 2.13360143\n",
      "Step: 61  Loss: 2.12727594\n",
      "Step: 62  Loss: 2.12079763\n",
      "Step: 63  Loss: 2.11416364\n",
      "Step: 64  Loss: 2.10737205\n",
      "Step: 65  Loss: 2.10042095\n",
      "Step: 66  Loss: 2.09330773\n",
      "Step: 67  Loss: 2.08603072\n",
      "Step: 68  Loss: 2.07858872\n",
      "Step: 69  Loss: 2.07097912\n",
      "Step: 70  Loss: 2.06320071\n",
      "Step: 71  Loss: 2.05525255\n",
      "Step: 72  Loss: 2.04713273\n",
      "Step: 73  Loss: 2.03884125\n",
      "Step: 74  Loss: 2.0303762\n",
      "Step: 75  Loss: 2.02173734\n",
      "Step: 76  Loss: 2.01292467\n",
      "Step: 77  Loss: 2.00393677\n",
      "Step: 78  Loss: 1.99477458\n",
      "Step: 79  Loss: 1.98543787\n",
      "Step: 80  Loss: 1.97592711\n",
      "Step: 81  Loss: 1.96624267\n",
      "Step: 82  Loss: 1.95638466\n",
      "Step: 83  Loss: 1.94635499\n",
      "Step: 84  Loss: 1.93615389\n",
      "Step: 85  Loss: 1.9257828\n",
      "Step: 86  Loss: 1.91524351\n",
      "Step: 87  Loss: 1.90453684\n",
      "Step: 88  Loss: 1.89366519\n",
      "Step: 89  Loss: 1.88262963\n",
      "Step: 90  Loss: 1.87143242\n",
      "Step: 91  Loss: 1.860075\n",
      "Step: 92  Loss: 1.84855938\n",
      "Step: 93  Loss: 1.83688784\n",
      "Step: 94  Loss: 1.82506168\n",
      "Step: 95  Loss: 1.81310856\n",
      "Step: 96  Loss: 1.80103898\n",
      "Step: 97  Loss: 1.78882849\n",
      "Step: 98  Loss: 1.77647924\n",
      "Step: 99  Loss: 1.76399291\n",
      "Step: 100  Loss: 1.75137138\n",
      "Step: 101  Loss: 1.73861754\n",
      "Step: 102  Loss: 1.72573161\n",
      "Step: 103  Loss: 1.71271515\n",
      "Step: 104  Loss: 1.69956875\n",
      "Step: 105  Loss: 1.68629396\n",
      "Step: 106  Loss: 1.67288983\n",
      "Step: 107  Loss: 1.65935743\n",
      "Step: 108  Loss: 1.64569604\n",
      "Step: 109  Loss: 1.6319052\n",
      "Step: 110  Loss: 1.61798429\n",
      "Step: 111  Loss: 1.60393226\n",
      "Step: 112  Loss: 1.58974814\n",
      "Step: 113  Loss: 1.57543075\n",
      "Step: 114  Loss: 1.56097925\n",
      "Step: 115  Loss: 1.54639292\n",
      "Step: 116  Loss: 1.53167081\n",
      "Step: 117  Loss: 1.51681364\n",
      "Step: 118  Loss: 1.50182128\n",
      "Step: 119  Loss: 1.48669529\n",
      "Step: 120  Loss: 1.47143805\n",
      "Step: 121  Loss: 1.45605326\n",
      "Step: 122  Loss: 1.44054484\n",
      "Step: 123  Loss: 1.42492008\n",
      "Step: 124  Loss: 1.40918696\n",
      "Step: 125  Loss: 1.39335597\n",
      "Step: 126  Loss: 1.37743938\n",
      "Step: 127  Loss: 1.36145234\n",
      "Step: 128  Loss: 1.34541202\n",
      "Step: 129  Loss: 1.32933962\n",
      "Step: 130  Loss: 1.31325793\n",
      "Step: 131  Loss: 1.29719329\n",
      "Step: 132  Loss: 1.28117514\n",
      "Step: 133  Loss: 1.26523507\n",
      "Step: 134  Loss: 1.24940872\n",
      "Step: 135  Loss: 1.23373306\n",
      "Step: 136  Loss: 1.21824765\n",
      "Step: 137  Loss: 1.20299435\n",
      "Step: 138  Loss: 1.18801594\n",
      "Step: 139  Loss: 1.17335606\n",
      "Step: 140  Loss: 1.15905857\n",
      "Step: 141  Loss: 1.14516699\n",
      "Step: 142  Loss: 1.13172317\n",
      "Step: 143  Loss: 1.11876655\n",
      "Step: 144  Loss: 1.10633492\n",
      "Step: 145  Loss: 1.09446084\n",
      "Step: 146  Loss: 1.08317363\n",
      "Step: 147  Loss: 1.07249725\n",
      "Step: 148  Loss: 1.06245017\n",
      "Step: 149  Loss: 1.05304515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[-0.20532076],\n",
       "       [-0.20532076],\n",
       "       [ 0.25336722],\n",
       "       [ 0.25336722]], dtype=float32)>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(weights, X, Y):\n",
    "    W1 = tf.Variable(weights[0])\n",
    "    B1 = tf.Variable(weights[1])\n",
    "    W2 = tf.Variable(weights[2])\n",
    "\n",
    "def sparsify_smallest(weights, rate):\n",
    "    def sparsify(w):\n",
    "        w_1d = tf.reshape(w, -1)\n",
    "        w_1d_sorted = tf.sort(w_1d)\n",
    "        threshold = w_1d_sorted[int(w_1d_sorted.shape[0] * rate)]\n",
    "\n",
    "        # Set the weights to zero if they are less than the threshold.\n",
    "        w_below_threshold = tf.math.less(w_1d_sorted, threshold)\n",
    "        w_below_threshold = tf.cast(w_below_threshold, tf.float32)\n",
    "        w_below_threshold = tf.reshape(w_below_threshold, w.shape)\n",
    "        w.assign(w * w_below_threshold)\n",
    "\n",
    "        return w\n",
    "\n",
    "    for w in weights:\n",
    "        sparsify(w)\n",
    "\n",
    "def compute_graph(x, W1, B1, W2, B2):\n",
    "    Yp = tf.matmul(x, W1) + B1\n",
    "    Yp = tf.nn.relu(Yp)\n",
    "    Yp = tf.matmul(Yp, W2) + B2\n",
    "    Yp = tf.tanh(Yp)\n",
    "\n",
    "    return Yp\n",
    "\n",
    "def train(steps=100, sparsify=False, sparsity_rate=0.4):\n",
    "    tf.random.set_seed(11)\n",
    "\n",
    "    W1 = tf.Variable(tf.random.normal((3, 30)))\n",
    "    B1 = tf.Variable(tf.squeeze(tf.random.normal([30])))\n",
    "\n",
    "    W2 = tf.Variable(tf.random.normal((30, 1)))\n",
    "    B2 = tf.Variable(tf.squeeze(tf.random.normal([1])))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    loss = None\n",
    "    losses = []\n",
    "\n",
    "    for step_id in range(steps):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            Yp = compute_graph(X, W1, B1, W2, B2)\n",
    "            loss = tf.reduce_mean(tf.square(Yp - Y))\n",
    "\n",
    "        gradients = tape.gradient(loss, [W1, B1, W2, B2])\n",
    "        opt.apply_gradients(zip(gradients, [W1, B1, W2, B2]))\n",
    "\n",
    "        sparsify_smallest([W1, B1, W2], sparsity_rate) if sparsify else None\n",
    "\n",
    "        tf.print('Step:', step_id, ' Loss:', loss)\n",
    "        losses.append(loss.numpy())\n",
    "    \n",
    "    return W1, B1, W2, B2, losses\n",
    "\n",
    "print('Regular Training')\n",
    "print('--------------------------------')\n",
    "W1, B1, W2, B2, losses = train(steps=500)\n",
    "plt.plot(losses, c='g', label='Regular')\n",
    "predictions = compute_graph(and_test_samples, W1, B1, W2, B2)\n",
    "predictions\n",
    "\n",
    "print('Sparsified Training')\n",
    "print('--------------------------------')\n",
    "W1, B1, W2, B2, losses_sparse = train(steps=500, sparsify=True)\n",
    "plt.plot(losses_sparse, c='b', label='Sparsified')\n",
    "predictions = compute_graph(and_test_samples, W1, B1, W2, B2)\n",
    "predictions\n",
    "\n",
    "# plt.plot(losses, c='g', label='Regular')\n",
    "# plt.plot(losses_sparse, c='b', label='Sparsified')\n",
    "\n",
    "# plt.title('Per Traning Step Losses')\n",
    "# plt.xlabel('Steps')\n",
    "# plt.ylabel('Loss Values')\n",
    "# plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[0.9999996 ],\n",
       "       [0.99999994],\n",
       "       [0.99999994],\n",
       "       [0.99999994]], dtype=float32)>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "and_test_samples = tf.constant(np.array([(0, 0, 0), (0, 1, 0), (1, 0, 0), (1, 1, 1)]).astype('float32'))\n",
    "\n",
    "# or_test_samples = [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 1)]\n",
    "# and_test_samples\n",
    "\n",
    "def compute_graph(x, W1, B1, W2, B2):\n",
    "    Yp = tf.matmul(x, W1) + B1\n",
    "    Yp = tf.nn.relu(Yp)\n",
    "    Yp = tf.matmul(Yp, W2) + B2\n",
    "    Yp = tf.tanh(Yp)\n",
    "\n",
    "    return Yp\n",
    "\n",
    "tf.random.set_seed(11)\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal((3, 100)))\n",
    "B1 = tf.Variable(tf.squeeze(tf.random.normal([100])))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal((100, 1)))\n",
    "B2 = tf.Variable(tf.squeeze(tf.random.normal([1])))\n",
    "\n",
    "predictions = compute_graph(and_test_samples, W1, B1, W2, B2)\n",
    "predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction to Sparsity.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000027?line=19'>20</a>\u001b[0m e \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000027?line=21'>22</a>\u001b[0m \u001b[39m# create model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000027?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mplaceholder(shape\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m], dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000027?line=23'>24</a>\u001b[0m W \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(tf\u001b[39m.\u001b[39mrandom_uniform([\u001b[39m16\u001b[39m, \u001b[39m4\u001b[39m], \u001b[39m0\u001b[39m, \u001b[39m0.1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000027?line=24'>25</a>\u001b[0m out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmatmul(x, W)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "# simple neural network implementation of qlearning\n",
    "import gym\n",
    "from gym import wrappers \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# build environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "# env = wrappers.Monitor(env, '/tmp/frozenlake-qlearning', force=True)\n",
    "n_obv = env.observation_space.n\n",
    "n_acts = env.action_space.n\n",
    "\n",
    "# initialization \n",
    "learning_rate = 0.1 \n",
    "gamma = 0.99 \n",
    "train_episodes = 10000\n",
    "episodes = 0 \n",
    "prev_state = env.reset() \n",
    "episode_t = 0\n",
    "e = 0.1 \n",
    "\n",
    "# create model\n",
    "x = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16, 4], 0, 0.1))\n",
    "out = tf.matmul(x, W)\n",
    "act = tf.argmax(out, 1)\n",
    "t = tf.placeholder(shape=[1, 4], dtype = tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(t - out))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "# start session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "while episodes < train_episodes: \n",
    "    episode_t += 1\n",
    "    # take noisy action \n",
    "    action, qvalues = sess.run([act, out], feed_dict={x: np.identity(16)[prev_state:prev_state + 1]})\n",
    "    if (np.random.rand(1)) < e: \n",
    "        action[0] = env.action_space.sample()\n",
    "    next_state, rew, done, _ = env.step(action[0])\n",
    "\n",
    "    # find targetQ values and update model\n",
    "    qnext_values = sess.run([out], feed_dict={x: np.identity(16)[next_state:next_state + 1]})\n",
    "    max_q = np.max(qnext_values)\n",
    "    targetq = qvalues \n",
    "    targetq[0, action[0]] = rew + gamma * max_q\n",
    "    sess.run([train_step], feed_dict={x: np.identity(16)[prev_state:prev_state + 1], t: targetq})\n",
    "    prev_state = next_state\n",
    "\n",
    "    # episode finished\n",
    "    if done: \n",
    "        episodes += 1\n",
    "        # decrease noise as number of episodes increases\n",
    "        e = 1./((episodes/50) + 10)\n",
    "        prev_state = env.reset()\n",
    "        print (\"episode %d finished after %d timesteps, rew = %d\" % (episodes, episode_t, rew))\n",
    "        episode_t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"FrozenLake-v1\")\n",
    "N_STATES = ENV.observation_space.n\n",
    "N_ACTIONS = ENV.action_space.n\n",
    "\n",
    "# initialization \n",
    "LEARNING_RATE = 0.1 \n",
    "GAMMA = 0.99 \n",
    "N_EPISODES = 1000\n",
    "\n",
    "\n",
    "# create model\n",
    "x = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16, 4], 0, 0.1))\n",
    "out = tf.matmul(x, W)\n",
    "act = tf.argmax(out, 1)\n",
    "t = tf.placeholder(shape=[1, 4], dtype = tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(t - out))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "# start session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "def train(steps=5):\n",
    "    Qo = tf.zeros((1, N_STATES))\n",
    "    W = tf.Variable(tf.random.uniform([N_STATES, N_ACTIONS]))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    loss = None\n",
    "    losses = []\n",
    "\n",
    "    for step_idx in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q = tf.matmul(Qo, W)\n",
    "            prediction = tf.argmax(Q, 1)\n",
    "            loss = tf.reduce_sum(tf.square(nextQ - Q))\n",
    "        \n",
    "        gradients = tape.gradient(loss, W)\n",
    "        opt.apply_gradients(zip(gradients, [W]))\n",
    "\n",
    "        tf.print('Step:', step_idx, ' Loss:', loss)\n",
    "        losses.append(loss.numpy())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "train()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple neural network implementation of qlearning\n",
    "import gym\n",
    "from gym import wrappers \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# build environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "# env = wrappers.Monitor(env, '/tmp/frozenlake-qlearning', force=True)\n",
    "n_obv = env.observation_space.n\n",
    "n_acts = env.action_space.n\n",
    "\n",
    "# initialization \n",
    "learning_rate = 0.1 \n",
    "gamma = 0.99 \n",
    "train_episodes = 10000\n",
    "episodes = 0 \n",
    "prev_state = env.reset() \n",
    "episode_t = 0\n",
    "e = 0.1 \n",
    "\n",
    "# create model\n",
    "x = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16, 4], 0, 0.1))\n",
    "out = tf.matmul(x, W)\n",
    "act = tf.argmax(out, 1)\n",
    "t = tf.placeholder(shape=[1, 4], dtype = tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(t - out))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "# start session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "while episodes < train_episodes: \n",
    "    episode_t += 1\n",
    "    # take noisy action \n",
    "    action, qvalues = sess.run([act, out], feed_dict={x: np.identity(16)[prev_state:prev_state + 1]})\n",
    "    if (np.random.rand(1)) < e: \n",
    "        action[0] = env.action_space.sample()\n",
    "    next_state, rew, done, _ = env.step(action[0])\n",
    "\n",
    "    # find targetQ values and update model\n",
    "    qnext_values = sess.run([out], feed_dict={x: np.identity(16)[next_state:next_state + 1]})\n",
    "    max_q = np.max(qnext_values)\n",
    "    targetq = qvalues \n",
    "    targetq[0, action[0]] = rew + gamma * max_q\n",
    "    sess.run([train_step], feed_dict={x: np.identity(16)[prev_state:prev_state + 1], t: targetq})\n",
    "    prev_state = next_state\n",
    "\n",
    "    # episode finished\n",
    "    if done: \n",
    "        episodes += 1\n",
    "        # decrease noise as number of episodes increases\n",
    "        e = 1./((episodes/50) + 10)\n",
    "        prev_state = env.reset()\n",
    "        print (\"episode %d finished after %d timesteps, rew = %d\" % (episodes, episode_t, rew))\n",
    "        episode_t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "construct_q_network() missing 2 required positional arguments: 'state_dim' and 'action_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction to Sparsity.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000031?line=20'>21</a>\u001b[0m     q_network \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39m[q_values])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000031?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m q_network\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/broxoli/projects/book/codelabs/Chapter-05-Compression-Techniques-Part-2/Introduction%20to%20Sparsity.ipynb#ch0000031?line=24'>25</a>\u001b[0m q_network \u001b[39m=\u001b[39m construct_q_network()\n",
      "\u001b[0;31mTypeError\u001b[0m: construct_q_network() missing 2 required positional arguments: 'state_dim' and 'action_dim'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, initializers, models\n",
    "from tensorflow import keras\n",
    "\n",
    "def construct_q_network(state_dim, action_dim):\n",
    "    \"\"\"Construct the q-network with q-values per action as output\"\"\"\n",
    "    inputs = layers.Input(shape=(state_dim,))  # input dimension\n",
    "    hidden1 = layers.Dense(\n",
    "        25, activation=\"relu\",)(inputs)\n",
    "    hidden2 = layers.Dense(\n",
    "        25, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
    "    )(hidden1)\n",
    "    hidden3 = layers.Dense(\n",
    "        25, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
    "    )(hidden2)\n",
    "    q_values = layers.Dense(\n",
    "        action_dim, kernel_initializer=initializers.Zeros(), activation=\"linear\"\n",
    "    )(\n",
    "        hidden3\n",
    "    )\n",
    "\n",
    "    q_network = keras.Model(inputs=inputs, outputs=[q_values])\n",
    "\n",
    "    return q_network\n",
    "\n",
    "q_network = construct_q_network()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======episode 0 ======\n",
      "Q-values ['0.000', '0.000', '0.000', '0.000']\n",
      "Rel. deviation ['-1.000', '-1.000', '-1.000', '-1.000']\n",
      "\n",
      "======episode 1000 ======\n",
      "Q-values ['0.898', '1.280', '0.972', '1.612']\n",
      "Rel. deviation ['-0.002', '0.066', '0.388', '0.612']\n",
      "\n",
      "======episode 2000 ======\n",
      "Q-values ['1.084', '1.279', '0.798', '0.775']\n",
      "Rel. deviation ['0.205', '0.066', '0.140', '-0.225']\n",
      "\n",
      "======episode 3000 ======\n",
      "Q-values ['0.721', '1.082', '0.345', '0.764']\n",
      "Rel. deviation ['-0.199', '-0.098', '-0.507', '-0.236']\n",
      "\n",
      "======episode 4000 ======\n",
      "Q-values ['0.929', '1.127', '0.744', '0.790']\n",
      "Rel. deviation ['0.032', '-0.061', '0.063', '-0.210']\n",
      "\n",
      "======episode 5000 ======\n",
      "Q-values ['0.842', '1.191', '0.791', '0.724']\n",
      "Rel. deviation ['-0.064', '-0.007', '0.130', '-0.276']\n",
      "\n",
      "======episode 6000 ======\n",
      "Q-values ['0.587', '1.212', '0.880', '1.045']\n",
      "Rel. deviation ['-0.348', '0.010', '0.257', '0.045']\n",
      "\n",
      "======episode 7000 ======\n",
      "Q-values ['0.747', '1.065', '0.637', '0.878']\n",
      "Rel. deviation ['-0.170', '-0.112', '-0.089', '-0.122']\n",
      "\n",
      "======episode 8000 ======\n",
      "Q-values ['1.213', '1.369', '0.744', '1.181']\n",
      "Rel. deviation ['0.348', '0.141', '0.063', '0.181']\n",
      "\n",
      "======episode 9000 ======\n",
      "Q-values ['1.020', '1.303', '0.667', '1.048']\n",
      "Rel. deviation ['0.133', '0.086', '-0.047', '0.048']\n",
      "\n",
      "======episode 10000 ======\n",
      "Q-values ['0.915', '1.161', '0.429', '0.799']\n",
      "Rel. deviation ['0.017', '-0.032', '-0.388', '-0.201']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Needed for training the network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.initializers as initializers\n",
    "\n",
    "# Needed for animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_reward(bandit: float) -> tf.Tensor:\n",
    "    \"\"\"Generate reward for selected bandit\"\"\"\n",
    "    reward = tf.random.normal([1], mean=bandit, stddev=1, dtype=tf.dtypes.float32)\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def plot(q_values: tf.Tensor, bandits: np.array) -> None:\n",
    "    \"\"\"Plot bar chart with selection probability per bandit\"\"\"\n",
    "    q_values_plot = [\n",
    "        q_values[0],\n",
    "        q_values[1],\n",
    "        q_values[2],\n",
    "        q_values[3],\n",
    "    ]\n",
    "    bandit_plot = [\n",
    "        bandits[0],\n",
    "        bandits[1],\n",
    "        bandits[2],\n",
    "        bandits[3],\n",
    "    ]\n",
    "    width = 0.4\n",
    "    x = np.arange(len(bandits))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x - width / 2, q_values_plot, width, label=\"Q-values\")\n",
    "    ax.bar(x + width / 2, bandit_plot, width, label=\"True values\")\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xticks([0, 1, 2, 3])\n",
    "    ax.set_xticklabels([\"1\", \"2\", \"3\", \"4\"])\n",
    "\n",
    "    plt.xlabel(\"Bandit\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def construct_q_network(state_dim: int, action_dim: int) -> keras.Model:\n",
    "    \"\"\"Construct the critic network with q-values per action as output\"\"\"\n",
    "    inputs = layers.Input(shape=(state_dim,))  # input dimension\n",
    "    hidden1 = layers.Dense(\n",
    "        10, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
    "    )(inputs)\n",
    "    hidden2 = layers.Dense(\n",
    "        10, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
    "    )(hidden1)\n",
    "    hidden3 = layers.Dense(\n",
    "        10, activation=\"relu\", kernel_initializer=initializers.he_normal()\n",
    "    )(hidden2)\n",
    "    q_values = layers.Dense(\n",
    "        action_dim, kernel_initializer=initializers.Zeros(), activation=\"linear\"\n",
    "    )(hidden3)\n",
    "\n",
    "    deep_q_network = keras.Model(inputs=inputs, outputs=[q_values])\n",
    "\n",
    "    return deep_q_network\n",
    "\n",
    "\n",
    "def mean_squared_error_loss(q_value: tf.Tensor, reward: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Compute mean squared error loss\"\"\"\n",
    "    loss = 0.5 * (q_value - reward) ** 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize parameters\n",
    "    state = tf.constant([[1]])\n",
    "    bandits = np.array([0.9, 1.2, 0.7, 1.0])\n",
    "    state_dim = len(state)\n",
    "    action_dim = len(bandits)\n",
    "    exploration_rate = 0.1\n",
    "    learning_rate = 0.01\n",
    "    num_episodes = 10000\n",
    "\n",
    "    # Construct Q-network\n",
    "    q_network = construct_q_network(state_dim, action_dim)\n",
    "\n",
    "    # Define optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    for i in range(num_episodes + 1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Obtain Q-values from network\n",
    "            q_values = q_network(state)\n",
    "\n",
    "            epsilon = np.random.rand()\n",
    "            if epsilon <= exploration_rate:\n",
    "                # Select random action\n",
    "                action = np.random.choice(len(bandits))\n",
    "            else:\n",
    "                # Select action with highest q-value\n",
    "                action = np.argmax(q_values)\n",
    "\n",
    "            # Obtain reward from bandit\n",
    "            reward = get_reward(bandits[action])\n",
    "\n",
    "            # Obtain Q-value\n",
    "            q_value = q_values[0, action]\n",
    "\n",
    "            # Compute loss value\n",
    "            loss_value = mean_squared_error_loss(q_value, reward)\n",
    "\n",
    "            # Compute gradients\n",
    "            grads = tape.gradient(loss_value[0], q_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update network weights\n",
    "            opt.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "\n",
    "            # Print console output\n",
    "            if np.mod(i, 1000) == 0:\n",
    "                print(\"\\n======episode\", i, \"======\")\n",
    "                print(\"Q-values\", [\"%.3f\" % n for n in q_values[0]])\n",
    "                print(\n",
    "                    \"Rel. deviation\",\n",
    "                    [\n",
    "                        \"%.3f\" % float((q_values[0, i] - bandits[i]) / bandits[i])\n",
    "                        for i in range(len(q_values[0]))\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "    # Plot Q-values\n",
    "    plot(q_values[0], bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 4 5 0.1\n",
      "tf.Tensor([[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(1, 16), dtype=float32)\n",
      "4 0.0 False\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(1, 16), dtype=float32)\n",
      "4 0.0 False\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(1, 16), dtype=float32)\n",
      "4 0.0 False\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(1, 16), dtype=float32)\n",
      "4 0.0 False\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(1, 16), dtype=float32)\n",
      "4 0.0 False\n",
      "tf.Tensor([[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(1, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Needed for training the network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.initializers as initializers\n",
    "\n",
    "# Needed for animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_reward(bandit: float) -> tf.Tensor:\n",
    "    \"\"\"Generate reward for selected bandit\"\"\"\n",
    "    reward = tf.random.normal([1], mean=bandit, stddev=1, dtype=tf.dtypes.float32)\n",
    "\n",
    "    return reward\n",
    "\n",
    "def construct_q_network(state_dim, action_dim):\n",
    "    \"\"\"Construct the critic network with q-values per action as output\"\"\"\n",
    "    inputs = layers.Input(shape=(state_dim))  # input dimension\n",
    "    hidden1 = layers.Dense(10, activation=\"relu\")(inputs)\n",
    "    hidden2 = layers.Dense(10, activation=\"relu\")(hidden1)\n",
    "    hidden3 = layers.Dense(10, activation=\"relu\")(hidden2)\n",
    "    q_values = layers.Dense(action_dim)(hidden3)\n",
    "\n",
    "    deep_q_network = keras.Model(inputs=inputs, outputs=[q_values])\n",
    "\n",
    "    return deep_q_network\n",
    "\n",
    "\n",
    "def mean_squared_error_loss(q_value, reward):\n",
    "    \"\"\"Compute mean squared error loss\"\"\"\n",
    "    loss = 0.5 * (q_value - reward) ** 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "ENV = gym.make(\"FrozenLake-v1\")\n",
    "N_STATES = ENV.observation_space.n\n",
    "N_ACTIONS = ENV.action_space.n\n",
    "N_EPISODES = 5\n",
    "EXPLORATION_RATE = .1\n",
    "\n",
    "print(N_STATES, N_ACTIONS, N_EPISODES, EXPLORATION_RATE)\n",
    "model = construct_q_network(N_STATES, N_ACTIONS)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "state = tf.expand_dims(tf.one_hot(ENV.reset(), N_STATES, dtype=tf.float32), 0)\n",
    "print(state)\n",
    "\n",
    "for ep_idx in range(N_EPISODES):\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model(state)\n",
    "\n",
    "        # Choose between the greey move or the random move.\n",
    "        # Greedy move take the greedy approach to chose the best current action.\n",
    "        epsilon = np.random.rand()\n",
    "        if epsilon <= EXPLORATION_RATE:\n",
    "            action = np.random.choice(N_ACTIONS)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        \n",
    "        # Take the action and get the next state and reward.\n",
    "        next_state, reward, done, _ = ENV.step(action)\n",
    "        print(next_state, reward, done)\n",
    "\n",
    "        # Convert the next state to one-hot encoding.\n",
    "        next_state = tf.expand_dims(tf.one_hot(next_state, N_STATES, dtype=tf.float32), 0)\n",
    "        print(next_state)\n",
    "\n",
    "        # Compute Q(s,a) and Q(s',a').\n",
    "        q_s_a = model(state)\n",
    "        q_s_a_next = model(next_state)\n",
    "\n",
    "        # Compute the target Q-value.\n",
    "\n",
    "\n",
    "\n",
    "        # Compute the loss.\n",
    "\n",
    "\n",
    "        loss = mean_squared_error_loss(q_values[0], reward)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize parameters\n",
    "#     state = tf.constant([[1]])\n",
    "#     bandits = np.array([0.9, 1.2, 0.7, 1.0])\n",
    "#     state_dim = len(state)\n",
    "#     action_dim = len(bandits)\n",
    "#     exploration_rate = 0.1\n",
    "#     learning_rate = 0.01\n",
    "#     num_episodes = 10000\n",
    "\n",
    "#     # Construct Q-network\n",
    "#     q_network = construct_q_network(state_dim, action_dim)\n",
    "\n",
    "#     # Define optimizer\n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "#     for i in range(num_episodes + 1):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             # Obtain Q-values from network\n",
    "#             q_values = q_network(state)\n",
    "\n",
    "#             epsilon = np.random.rand()\n",
    "#             if epsilon <= exploration_rate:\n",
    "#                 # Select random action\n",
    "#                 action = np.random.choice(len(bandits))\n",
    "#             else:\n",
    "#                 # Select action with highest q-value\n",
    "#                 action = np.argmax(q_values)\n",
    "\n",
    "#             # Obtain reward from bandit\n",
    "#             reward = get_reward(bandits[action])\n",
    "\n",
    "#             # Obtain Q-value\n",
    "#             q_value = q_values[0, action]\n",
    "\n",
    "#             # Compute loss value\n",
    "#             loss_value = mean_squared_error_loss(q_value, reward)\n",
    "\n",
    "#             # Compute gradients\n",
    "#             grads = tape.gradient(loss_value[0], q_network.trainable_variables)\n",
    "\n",
    "#             # Apply gradients to update network weights\n",
    "#             opt.apply_gradients(zip(grads, q_network.trainable_variables))\n",
    "\n",
    "#             # Print console output\n",
    "#             if np.mod(i, 1000) == 0:\n",
    "#                 print(\"\\n======episode\", i, \"======\")\n",
    "#                 print(\"Q-values\", [\"%.3f\" % n for n in q_values[0]])\n",
    "#                 print(\n",
    "#                     \"Rel. deviation\",\n",
    "#                     [\n",
    "#                         \"%.3f\" % float((q_values[0, i] - bandits[i]) / bandits[i])\n",
    "#                         for i in range(len(q_values[0]))\n",
    "#                     ],\n",
    "#                 )\n",
    "\n",
    "#     # Plot Q-values\n",
    "#     plot(q_values[0], bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple neural network implementation of qlearning\n",
    "import gym\n",
    "\n",
    "# build environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.reset()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef4ac4ea1ec422be6b4eb59e3fa0ded4ce016edaf83e8378f1dbc473945965d7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
