{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Hyperparameter Optimization\n",
    "In this exercise, we tune the *layer_size* and *learning_rate* hyperparameters. The *layer_size* parameter decides the number of units in the hidden layer and the *learning_rate* parameter is an optimizer configuration.\n",
    "\n",
    "#### We try the following values:\n",
    "\n",
    "| *layer_size* | *learning_rate* |\n",
    "| --- | --- |\n",
    "| 5 | .01 |\n",
    "| 10 | .1 |\n",
    "| 20 | .001 |\n",
    "| 50 | .1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers, losses, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0 learning_rate: 0.01 layer_size: 5 loss: 0.1583523005247116\n",
      "Trial: 1 learning_rate: 0.1 layer_size: 10 loss: 0.12014190107584\n",
      "Trial: 2 learning_rate: 0.01 layer_size: 20 loss: 0.1500110924243927\n",
      "Trial: 3 learning_rate: 0.01 layer_size: 50 loss: 0.15035775303840637\n",
      "\n",
      "=============== Search Summary ===============\n",
      "Best Trial: 1 Loss: 0.12014190107584\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform((20, 5))\n",
    "Y = tf.squeeze(\n",
    "    tf.one_hot(tf.random.uniform((20, 1), 0, 5, tf.int64), 5)\n",
    ")\n",
    "\n",
    "# A set of pairs of layer size and learning rate to run HPO.\n",
    "S = [\n",
    "    (5, .01),\n",
    "    (10, .1),\n",
    "    (20, .001),\n",
    "    (50, .1),\n",
    "]\n",
    "\n",
    "def create_model(size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(5,5)),\n",
    "        layers.Dense(size, activation='relu'),\n",
    "        layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "\n",
    "SEARCH_RESULTS = []\n",
    "\n",
    "for trial_id, (layer_size, learning_rate) in enumerate(S):\n",
    "    model = create_model(size=layer_size)\n",
    "    opt = optimizers.SGD(learning_rate=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    for iteration in range(2000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = model(X)\n",
    "            loss = tf.reduce_mean(tf.math.square(Y - output))\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            losses.append(loss.numpy())\n",
    "\n",
    "    min_loss = np.min(losses)\n",
    "    SEARCH_RESULTS.append(min_loss)\n",
    "    fmt = 'Trial: {} learning_rate: {} layer_size: {} loss: {}'\n",
    "    print(fmt.format(trial_id, learning_rate, layer_size, min_loss))\n",
    "\n",
    "best_trial_id = np.argmin(SEARCH_RESULTS)\n",
    "best_loss = np.min(SEARCH_RESULTS)\n",
    "\n",
    "print('\\n=============== Search Summary ===============')\n",
    "print('Best Trial: {} Loss: {}'.format(best_trial_id, best_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef4ac4ea1ec422be6b4eb59e3fa0ded4ce016edaf83e8378f1dbc473945965d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
